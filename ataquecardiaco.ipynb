{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122400ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Tech Challenge - Fase 1: Predi√ß√£o de Ataque Card√≠aco\n",
    "\n",
    "## An√°lise de Dados e Machine Learning para Diagn√≥stico M√©dico\n",
    "\n",
    "**Objetivo:** Criar um sistema de apoio ao diagn√≥stico m√©dico que utilize t√©cnicas de Machine Learning para estimar a probabilidade de ocorr√™ncia de um ataque card√≠aco a partir de informa√ß√µes cl√≠nicas e demogr√°ficas de pacientes.\n",
    "\n",
    "**Base de Dados:** Heart Attack Prediction - Indonesia\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introdu√ß√£o e Contexto do Problema\n",
    "\n",
    "As doen√ßas cardiovasculares continuam sendo uma das principais causas de morte em todo o mundo. Milhares de pessoas perdem a vida todos os anos em decorr√™ncia de eventos card√≠acos que, em muitos casos, poderiam ser evitados com uma detec√ß√£o precoce e um acompanhamento adequado.\n",
    "\n",
    "Neste projeto, buscamos desenvolver um modelo preditivo capaz de identificar pacientes com maior risco de sofrer um ataque card√≠aco. A proposta √© aplicar t√©cnicas de Machine Learning sobre dados cl√≠nicos estruturados, de forma a extrair padr√µes e indicadores relevantes que auxiliem no processo de triagem e tomada de decis√£o m√©dica.\n",
    "\n",
    "Com isso o dataset apresenta cen√°rios com diversas variaveis e a variavel alvo diz se ele teve ataque cardiaco ou n√£o!\n",
    "\n",
    "Ao automatizar parte dessa an√°lise, espera-se reduzir o tempo necess√°rio para a identifica√ß√£o de casos cr√≠ticos e apoiar profissionais da sa√∫de no planejamento de interven√ß√µes preventivas mais eficazes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd07b37",
   "metadata": {},
   "source": [
    "## 2. Bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db076d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas numpy matplotlib seaborn scikit-learn shap\n",
    "\n",
    "# Manipula√ß√£o de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualiza√ß√£o\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Exibi√ß√£o de dataframes\n",
    "from IPython.display import display\n",
    "\n",
    "# Pr√©-processamento\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelos de Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# M√©tricas de avalia√ß√£o\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Configura√ß√µes\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec744d2c",
   "metadata": {},
   "source": [
    "## 3. Carregamento e Explora√ß√£o Inicial dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55xiim5arii",
   "metadata": {},
   "source": [
    "### 3.1 Objetivos da Explora√ß√£o Inicial\n",
    "\n",
    "Nesta se√ß√£o, realizaremos o carregamento do dataset e uma explora√ß√£o inicial para compreender:\n",
    "\n",
    "**1. Dimensionalidade dos Dados:**\n",
    "- Quantidade de registros (linhas) dispon√≠veis para an√°lise\n",
    "- N√∫mero de vari√°veis (colunas) que descrevem cada paciente\n",
    "- Consumo de mem√≥ria do dataset para planejamento de processamento\n",
    "\n",
    "**2. Qualidade dos Dados:**\n",
    "- Identifica√ß√£o de valores ausentes (missing values)\n",
    "- Verifica√ß√£o de tipos de dados de cada coluna\n",
    "- Detec√ß√£o de poss√≠veis inconsist√™ncias\n",
    "\n",
    "**3. Primeiras Impress√µes:**\n",
    "- Visualiza√ß√£o das primeiras e √∫ltimas linhas do dataset\n",
    "- Entendimento das vari√°veis dispon√≠veis\n",
    "- Identifica√ß√£o da vari√°vel alvo (heart_attack)\n",
    "\n",
    "Essa etapa √© fundamental para planejar as estrat√©gias de limpeza e pr√©-processamento que ser√£o aplicadas posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c73756",
   "metadata": {},
   "source": [
    "#dados tecnicos, tamanho, colunas, quanto vai alocar de ram para eu trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset\n",
    "df = pd.read_csv('./dataset/heart_attack_prediction_indonesia.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a55fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INFORMA√á√ïES GERAIS DO DATASET\")\n",
    "print(f\"Dimens√µes do dataset: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
    "print(f\"Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "# --- Informa√ß√µes Gerais do DataFrame ---\n",
    "print(\"\\n--- Informa√ß√µes do DataFrame ---\")\n",
    "df.info()\n",
    "\n",
    "# --- Tipos de Dados ---\n",
    "print(\"\\n--- Tipos de Dados ---\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# --- Primeiras e √öltimas Linhas ---\n",
    "print(\"\\n--- Primeiras 5 Linhas ---\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n--- √öltimas 5 Linhas ---\")\n",
    "display(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Informa√ß√µes sobre tipos de dados e valores nulos\n",
    "print(\"INFORMA√á√ïES SOBRE COLUNAS E TIPOS DE DADOS\")\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos, elementos que posso tirar, pois atrapalhariam em minha analise.\n",
    "\n",
    "print(\"AN√ÅLISE DE VALORES AUSENTES\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Ausentes': missing_values,\n",
    "    'Percentual (%)': missing_percentage\n",
    "})\n",
    "missing_df = missing_df[missing_df['Valores Ausentes'] > 0].sort_values('Valores Ausentes', ascending=False)\n",
    "\n",
    "\n",
    "print(missing_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ng2c6d174hq",
   "metadata": {},
   "source": [
    "### 3.2 An√°lise de Valores Ausentes e Estrat√©gia de Tratamento\n",
    "\n",
    "**Achados Importantes:**\n",
    "\n",
    "A an√°lise revelou que a vari√°vel `alcohol_consumption` possui **59,9% de valores ausentes** (94.848 registros), o que representa um desafio significativo.\n",
    "\n",
    "**Estrat√©gias de Tratamento Consideradas:**\n",
    "\n",
    "1. **Remo√ß√£o da coluna**: Se a vari√°vel n√£o for cr√≠tica para o modelo\n",
    "2. **Imputa√ß√£o**: Preencher com valores baseados em estat√≠sticas (moda, mediana)\n",
    "3. **Categoria especial**: Criar uma categoria \"N√£o informado\" ou \"Unknown\"\n",
    "4. **An√°lise de padr√£o**: Verificar se a aus√™ncia segue algum padr√£o sistem√°tico\n",
    "\n",
    "**Decis√£o Adotada:**\n",
    "\n",
    "Considerando que o consumo de √°lcool √© um fator de risco cardiovascular conhecido, **optaremos por imputar os valores ausentes com a moda (valor mais frequente)** e criar uma flag indicadora de imputa√ß√£o para que o modelo possa aprender se a aus√™ncia √© informativa.\n",
    "\n",
    "Al√©m disso, verificaremos se existem outliers ou valores inconsistentes nas vari√°veis num√©ricas que possam impactar a qualidade do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbkr62ziffu",
   "metadata": {},
   "source": [
    "## 3.3 Limpeza e Tratamento de Dados\n",
    "\n",
    "A qualidade dos dados √© fundamental para o sucesso de qualquer modelo de Machine Learning. Nesta se√ß√£o, aplicaremos t√©cnicas de limpeza para garantir que nosso dataset esteja pronto para an√°lise e modelagem.\n",
    "\n",
    "**Etapas de Limpeza:**\n",
    "\n",
    "1. **Tratamento de Valores Ausentes**: Imputa√ß√£o da vari√°vel alcohol_consumption\n",
    "2. **Detec√ß√£o de Duplicatas**: Verificar e remover registros duplicados\n",
    "3. **Tratamento de Outliers**: Identificar e tratar valores extremos nas vari√°veis num√©ricas\n",
    "4. **Valida√ß√£o de Consist√™ncia**: Verificar se os valores est√£o dentro de faixas esperadas\n",
    "\n",
    "Cada uma dessas etapas ser√° executada com justificativa clara da abordagem adotada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g05luvqc4fi",
   "metadata": {},
   "source": [
    "### 3.3.1 Tratamento de Valores Ausentes\n",
    "\n",
    "**Abordagem:**\n",
    "- Para a vari√°vel `alcohol_consumption`, faremos imputa√ß√£o com a moda (categoria mais frequente)\n",
    "- Criaremos uma flag indicadora `alcohol_missing` para registrar onde houve imputa√ß√£o\n",
    "- Isso permite que o modelo aprenda se a aus√™ncia de informa√ß√£o √©, por si s√≥, um padr√£o relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6atm68asvg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma c√≥pia do dataframe para limpeza\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRATAMENTO DE VALORES AUSENTES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Criar flag indicadora de valores ausentes para alcohol_consumption\n",
    "df_clean['alcohol_missing'] = df_clean['alcohol_consumption'].isnull().astype(int)\n",
    "\n",
    "print(f\"\\n‚úì Flag 'alcohol_missing' criada:\")\n",
    "print(f\"  - Registros com dados ausentes: {df_clean['alcohol_missing'].sum()}\")\n",
    "print(f\"  - Percentual: {(df_clean['alcohol_missing'].sum() / len(df_clean)) * 100:.2f}%\")\n",
    "\n",
    "# 2. Imputar valores ausentes com a moda (categoria mais frequente)\n",
    "if df_clean['alcohol_consumption'].isnull().sum() > 0:\n",
    "    # Calcular a moda (categoria mais frequente)\n",
    "    mode_value = df_clean['alcohol_consumption'].mode()[0]\n",
    "    \n",
    "    # Imputar valores ausentes\n",
    "    df_clean['alcohol_consumption'].fillna(mode_value, inplace=True)\n",
    "    \n",
    "    print(f\"\\n‚úì Valores ausentes imputados com a moda: '{mode_value}'\")\n",
    "    print(f\"  - Valores ausentes restantes: {df_clean['alcohol_consumption'].isnull().sum()}\")\n",
    "\n",
    "# 3. Verificar se existem outros valores ausentes no dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICA√á√ÉO FINAL DE VALORES AUSENTES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_final = df_clean.isnull().sum()\n",
    "if missing_final.sum() == 0:\n",
    "    print(\"\\n‚úì Nenhum valor ausente encontrado no dataset!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Valores ausentes restantes:\")\n",
    "    print(missing_final[missing_final > 0])\n",
    "\n",
    "print(f\"\\nüìä Dimens√µes do dataset ap√≥s tratamento: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jaderxxmp7",
   "metadata": {},
   "source": [
    "### 3.3.2 Detec√ß√£o e Remo√ß√£o de Duplicatas\n",
    "\n",
    "**Por que remover duplicatas?**\n",
    "\n",
    "Registros duplicados podem:\n",
    "- Enviesar o modelo ao dar peso excessivo a certos padr√µes\n",
    "- Inflar artificialmente m√©tricas de desempenho\n",
    "- Causar vazamento de dados (data leakage) entre treino e teste\n",
    "\n",
    "**Abordagem:**\n",
    "Verificaremos se existem registros completamente duplicados e os removeremos, mantendo apenas a primeira ocorr√™ncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rl7jlbz7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DETEC√á√ÉO E REMO√á√ÉO DE DUPLICATAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Contar registros duplicados\n",
    "duplicates_count = df_clean.duplicated().sum()\n",
    "print(f\"\\nüìä Registros duplicados encontrados: {duplicates_count}\")\n",
    "\n",
    "if duplicates_count > 0:\n",
    "    # Calcular percentual\n",
    "    duplicates_percent = (duplicates_count / len(df_clean)) * 100\n",
    "    print(f\"   Percentual de duplicatas: {duplicates_percent:.2f}%\")\n",
    "    \n",
    "    # Remover duplicatas\n",
    "    df_clean = df_clean.drop_duplicates(keep='first')\n",
    "    print(f\"\\n‚úì Duplicatas removidas! Mantida a primeira ocorr√™ncia de cada registro.\")\n",
    "    print(f\"   Registros restantes: {len(df_clean)}\")\n",
    "else:\n",
    "    print(\"\\n‚úì Nenhum registro duplicado encontrado!\")\n",
    "\n",
    "print(f\"\\nüìä Dimens√µes finais do dataset: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpowzxx83mn",
   "metadata": {},
   "source": [
    "### 3.3.3 Detec√ß√£o e Tratamento de Outliers\n",
    "\n",
    "**O que s√£o outliers?**\n",
    "\n",
    "Outliers s√£o valores extremamente discrepantes que podem:\n",
    "- Representar erros de medi√ß√£o ou digita√ß√£o\n",
    "- Ser casos raros mas v√°lidos (ex: idade de 120 anos)\n",
    "- Distorcer o treinamento do modelo\n",
    "\n",
    "**M√©todos de Detec√ß√£o:**\n",
    "\n",
    "1. **M√©todo IQR (Interquartile Range)**: \n",
    "   - Detecta valores fora do intervalo [Q1 - 1.5√óIQR, Q3 + 1.5√óIQR]\n",
    "   - Robusto e n√£o assume distribui√ß√£o normal\n",
    "\n",
    "2. **An√°lise de Dom√≠nio**:\n",
    "   - Verificar se valores est√£o dentro de faixas biologicamente plaus√≠veis\n",
    "   - Ex: Idade entre 0-120 anos, press√£o arterial > 0, etc.\n",
    "\n",
    "**Abordagem Adotada:**\n",
    "\n",
    "Para vari√°veis cl√≠nicas, n√£o removeremos outliers automaticamente, pois valores extremos podem ser clinicamente relevantes (ex: colesterol muito alto). Em vez disso:\n",
    "- Identificaremos e reportaremos outliers\n",
    "- Deixaremos a decis√£o de tratamento para an√°lise caso a caso\n",
    "- Para outliers claramente imposs√≠veis (ex: idade negativa), faremos corre√ß√£o ou remo√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dgiznpdl2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DETEC√á√ÉO DE OUTLIERS - M√âTODO IQR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar vari√°veis num√©ricas (excluindo flags e target)\n",
    "numerical_cols_outliers = df_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols_outliers = [col for col in numerical_cols_outliers if col not in ['heart_attack', 'alcohol_missing']]\n",
    "\n",
    "# Fun√ß√£o para detectar outliers usando IQR\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'outliers_count': len(outliers),\n",
    "        'outliers_percent': (len(outliers) / len(data)) * 100,\n",
    "        'Q1': Q1,\n",
    "        'Q3': Q3,\n",
    "        'IQR': IQR\n",
    "    }\n",
    "\n",
    "# Detectar outliers em todas as vari√°veis num√©ricas\n",
    "outliers_report = []\n",
    "for col in numerical_cols_outliers:\n",
    "    outlier_info = detect_outliers_iqr(df_clean, col)\n",
    "    outliers_report.append(outlier_info)\n",
    "\n",
    "# Criar DataFrame com o relat√≥rio\n",
    "outliers_df = pd.DataFrame(outliers_report)\n",
    "outliers_df = outliers_df.sort_values('outliers_percent', ascending=False)\n",
    "\n",
    "# Exibir relat√≥rio resumido\n",
    "print(\"\\nüìä RELAT√ìRIO DE OUTLIERS (Top 10 vari√°veis com mais outliers):\\n\")\n",
    "print(outliers_df[['column', 'outliers_count', 'outliers_percent']].head(10).to_string(index=False))\n",
    "\n",
    "# Estat√≠sticas gerais\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ESTAT√çSTICAS GERAIS DE OUTLIERS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total de vari√°veis analisadas: {len(numerical_cols_outliers)}\")\n",
    "print(f\"Vari√°veis com outliers (>5%): {len(outliers_df[outliers_df['outliers_percent'] > 5])}\")\n",
    "print(f\"M√©dia de outliers por vari√°vel: {outliers_df['outliers_percent'].mean():.2f}%\")\n",
    "\n",
    "# Verificar valores imposs√≠veis (valida√ß√£o de dom√≠nio)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VALIDA√á√ÉO DE DOM√çNIO - VALORES IMPOSS√çVEIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "validation_issues = []\n",
    "\n",
    "# Idade negativa ou maior que 120\n",
    "if 'age' in df_clean.columns:\n",
    "    invalid_age = df_clean[(df_clean['age'] < 0) | (df_clean['age'] > 120)]\n",
    "    if len(invalid_age) > 0:\n",
    "        validation_issues.append(f\"‚ö†Ô∏è age: {len(invalid_age)} valores fora da faixa v√°lida (0-120)\")\n",
    "    else:\n",
    "        print(\"‚úì age: Todos os valores est√£o dentro da faixa v√°lida (0-120)\")\n",
    "\n",
    "# Press√£o arterial negativa\n",
    "for col in ['blood_pressure_systolic', 'blood_pressure_diastolic']:\n",
    "    if col in df_clean.columns:\n",
    "        invalid_bp = df_clean[df_clean[col] < 0]\n",
    "        if len(invalid_bp) > 0:\n",
    "            validation_issues.append(f\"‚ö†Ô∏è {col}: {len(invalid_bp)} valores negativos\")\n",
    "        else:\n",
    "            print(f\"‚úì {col}: Sem valores negativos\")\n",
    "\n",
    "# Horas de sono negativas ou maiores que 24\n",
    "if 'sleep_hours' in df_clean.columns:\n",
    "    invalid_sleep = df_clean[(df_clean['sleep_hours'] < 0) | (df_clean['sleep_hours'] > 24)]\n",
    "    if len(invalid_sleep) > 0:\n",
    "        validation_issues.append(f\"‚ö†Ô∏è sleep_hours: {len(invalid_sleep)} valores fora da faixa v√°lida (0-24)\")\n",
    "    else:\n",
    "        print(\"‚úì sleep_hours: Todos os valores est√£o dentro da faixa v√°lida (0-24)\")\n",
    "\n",
    "if validation_issues:\n",
    "    print(\"\\n‚ö†Ô∏è PROBLEMAS ENCONTRADOS:\")\n",
    "    for issue in validation_issues:\n",
    "        print(f\"  {issue}\")\n",
    "else:\n",
    "    print(\"\\n‚úì Nenhum valor imposs√≠vel detectado!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUS√ÉO: Outliers identificados mas n√£o removidos\")\n",
    "print(\"Justificativa: Em dados cl√≠nicos, valores extremos podem ser clinicamente\")\n",
    "print(\"significativos e devem ser mantidos para an√°lise m√©dica.\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dml993m65",
   "metadata": {},
   "source": [
    "### 3.3.4 Resumo da Limpeza de Dados\n",
    "\n",
    "Ap√≥s aplicar todas as t√©cnicas de limpeza, vamos atualizar nosso dataset principal e resumir as transforma√ß√µes realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324s3a315qs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o dataframe principal\n",
    "df = df_clean.copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESUMO DA LIMPEZA DE DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n‚úì TRANSFORMA√á√ïES REALIZADAS:\")\n",
    "print(f\"  1. Imputa√ß√£o de valores ausentes em 'alcohol_consumption'\")\n",
    "print(f\"  2. Cria√ß√£o de flag 'alcohol_missing' para rastrear imputa√ß√µes\")\n",
    "print(f\"  3. Remo√ß√£o de registros duplicados (se houver)\")\n",
    "print(f\"  4. Detec√ß√£o e an√°lise de outliers (mantidos para an√°lise cl√≠nica)\")\n",
    "\n",
    "print(f\"\\nüìä DATASET FINAL AP√ìS LIMPEZA:\")\n",
    "print(f\"  - Dimens√µes: {df.shape[0]} linhas √ó {df.shape[1]} colunas\")\n",
    "print(f\"  - Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  - Valores ausentes: {df.isnull().sum().sum()}\")\n",
    "print(f\"  - Registros duplicados: {df.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\n‚úì Dataset limpo e pronto para an√°lise explorat√≥ria detalhada!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entender a distribui√ß√£o da vari√°vel alvo 'heart_attack'\n",
    "\n",
    "target_counts = df['heart_attack'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(target_counts, labels=['N√£o', 'Sim'], autopct='%1.1f%%',\n",
    "        startangle=90, colors=colors, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "plt.title('Propor√ß√£o de Casos de Ataque Card√≠aco', fontsize=14, fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xp43eh4e96",
   "metadata": {},
   "source": [
    "### 3.4 Distribui√ß√£o da Vari√°vel Alvo\n",
    "\n",
    "**Import√¢ncia da An√°lise de Balanceamento:**\n",
    "\n",
    "A vari√°vel alvo `heart_attack` indica se o paciente sofreu um ataque card√≠aco (1) ou n√£o (0). √â crucial verificar se as classes est√£o balanceadas, pois:\n",
    "\n",
    "- **Dataset desbalanceado** pode fazer o modelo tender a prever sempre a classe majorit√°ria\n",
    "- **Acur√°cia pode ser enganosa** em casos de desbalanceamento severo\n",
    "- **T√©cnicas especiais** (SMOTE, class weights, etc.) podem ser necess√°rias se o desbalanceamento for significativo\n",
    "\n",
    "**Interpreta√ß√£o:**\n",
    "- Balanceamento ideal: pr√≥ximo de 50%-50%\n",
    "- Desbalanceamento leve: 60%-40%\n",
    "- Desbalanceamento moderado: 70%-30%\n",
    "- Desbalanceamento severo: acima de 80%-20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae57990",
   "metadata": {},
   "source": [
    "## 4. An√°lise Explorat√≥ria de Dados (EDA)\n",
    "\n",
    "A An√°lise Explorat√≥ria de Dados √© uma etapa crucial para entender os padr√µes, rela√ß√µes e caracter√≠sticas do dataset antes de aplicar modelos de Machine Learning.\n",
    "\n",
    "**Objetivos da EDA:**\n",
    "\n",
    "1. **Identificar rela√ß√µes entre vari√°veis e o target**: Quais fatores mais influenciam o risco de ataque card√≠aco?\n",
    "2. **Detectar padr√µes e tend√™ncias**: Existem grupos de risco espec√≠ficos?\n",
    "3. **Validar hip√≥teses cl√≠nicas**: Os dados confirmam conhecimentos m√©dicos estabelecidos?\n",
    "4. **Preparar para feature engineering**: Identificar oportunidades de criar novas vari√°veis\n",
    "\n",
    "### 4.1 An√°lise de Vari√°veis Categ√≥ricas\n",
    "\n",
    "**Por que analisar vari√°veis categ√≥ricas?**\n",
    "\n",
    "Vari√°veis categ√≥ricas como `gender`, `smoking_status`, `dietary_habits` etc. frequentemente apresentam forte associa√ß√£o com desfechos cl√≠nicos. A an√°lise cruzada dessas vari√°veis com o target nos permite:\n",
    "\n",
    "- Identificar **grupos de alto risco** (ex: fumantes, sedent√°rios)\n",
    "- Entender **fatores modific√°veis** que podem ser alvo de interven√ß√µes\n",
    "- Visualizar **propor√ß√µes de casos positivos** em cada categoria\n",
    "\n",
    "**Interpreta√ß√£o dos gr√°ficos:**\n",
    "- Barras vermelhas: percentual de pacientes COM ataque card√≠aco\n",
    "- Barras verdes: percentual de pacientes SEM ataque card√≠aco\n",
    "- Quanto maior a barra vermelha, maior o risco associado √†quela categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identificar vari√°veis categ√≥ricas ou de agrupamento \n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Vari√°veis Categ√≥ricas:\")\n",
    "print(categorical_cols)\n",
    "print(f\"\\nTotal: {len(categorical_cols)} vari√°veis categorizadoras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de vari√°veis categ√≥ricas vs target\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    if col in df.columns:\n",
    "        cross_tab = pd.crosstab(df[col], df['heart_attack'], normalize='index') * 100\n",
    "        cross_tab.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'])\n",
    "        axes[idx].set_title(f'Taxa de Ataque Card√≠aco por {col.replace(\"_\", \" \").title()}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(col.replace('_', ' ').title(), fontsize=10)\n",
    "        axes[idx].set_ylabel('Percentual (%)', fontsize=10)\n",
    "        axes[idx].legend(['N√£o', 'Sim'], title='Ataque Card√≠aco')\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031a5cc",
   "metadata": {},
   "source": [
    "### 4.2 An√°lise de Vari√°veis Num√©ricas\n",
    "\n",
    "**Import√¢ncia das Vari√°veis Num√©ricas:**\n",
    "\n",
    "Vari√°veis num√©ricas como idade, n√≠veis de colesterol, press√£o arterial e glicemia fornecem medidas quantitativas importantes para predi√ß√£o de risco cardiovascular.\n",
    "\n",
    "**An√°lises Realizadas:**\n",
    "\n",
    "1. **Histogramas com separa√ß√£o por target:**\n",
    "   - Distribui√ß√£o Verde: pacientes SEM ataque card√≠aco\n",
    "   - Distribui√ß√£o Vermelha: pacientes COM ataque card√≠aco\n",
    "   - **Objetivo**: Identificar se h√° diferen√ßa nas distribui√ß√µes entre os grupos\n",
    "\n",
    "2. **Boxplots por target:**\n",
    "   - Detectam outliers (pontos fora das caixas)\n",
    "   - Mostram medianas (linha vermelha) e quartis (caixas)\n",
    "   - **Objetivo**: Comparar valores t√≠picos entre pacientes com e sem ataque card√≠aco\n",
    "\n",
    "**O que procurar:**\n",
    "- **Separa√ß√£o clara** entre distribui√ß√µes ‚Üí vari√°vel potencialmente preditiva\n",
    "- **Sobreposi√ß√£o total** ‚Üí vari√°vel menos √∫til para predi√ß√£o\n",
    "- **Outliers** ‚Üí casos extremos que podem ser clinicamente relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480172d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Identificar vari√°veis num√©ricas \n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols.remove('heart_attack')  # Remover a vari√°vel comparativa \n",
    "\n",
    "print(\"Vari√°veis Num√©ricas:\")\n",
    "print(numerical_cols)\n",
    "print(f\"\\nTotal: {len(numerical_cols)} vari√°veis num√©ricas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribui√ß√£o das principais vari√°veis num√©ricas\n",
    "key_numerical = ['age', 'cholesterol_level', 'waist_circumference', 'blood_pressure_systolic',\n",
    "                'blood_pressure_diastolic', 'fasting_blood_sugar', 'cholesterol_hdl',\n",
    "                'cholesterol_ldl', 'triglycerides', 'sleep_hours']\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(key_numerical):\n",
    "    if col in df.columns:\n",
    "        # Histograma com KDE\n",
    "        df[df['heart_attack'] == 0][col].hist(ax=axes[idx], bins=30, alpha=0.6, \n",
    "                                               label='Sem Ataque', color='#2ecc71', density=True)\n",
    "        df[df['heart_attack'] == 1][col].hist(ax=axes[idx], bins=30, alpha=0.6, \n",
    "                                               label='Com Ataque', color='#e74c3c', density=True)\n",
    "        \n",
    "        axes[idx].set_title(f'Distribui√ß√£o: {col.replace(\"_\", \" \").title()}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(col.replace('_', ' ').title(), fontsize=10)\n",
    "        axes[idx].set_ylabel('Densidade', fontsize=10)\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('numerical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ccffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para detectar outliers\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(key_numerical):\n",
    "    if col in df.columns:\n",
    "        df.boxplot(column=col, by='heart_attack', ax=axes[idx], \n",
    "                  patch_artist=True, \n",
    "                  boxprops=dict(facecolor='lightblue'),\n",
    "                  medianprops=dict(color='red', linewidth=2))\n",
    "        axes[idx].set_title(f'Boxplot: {col.replace(\"_\", \" \").title()}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Ataque Card√≠aco (0 = N√£o, 1 = Sim)', fontsize=10)\n",
    "        axes[idx].set_ylabel(col.replace('_', ' ').title(), fontsize=10)\n",
    "        plt.sca(axes[idx])\n",
    "        plt.xticks([1, 2], ['N√£o', 'Sim'])\n",
    "\n",
    "plt.suptitle('')  # Remove o t√≠tulo autom√°tico do pandas\n",
    "plt.tight_layout()\n",
    "#plt.savefig('numerical_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79f5cc",
   "metadata": {},
   "source": [
    "### 4.3 An√°lise de Correla√ß√£o\n",
    "\n",
    "**O que √© Correla√ß√£o?**\n",
    "\n",
    "A correla√ß√£o mede a for√ßa e dire√ß√£o da rela√ß√£o linear entre duas vari√°veis, variando de -1 a +1:\n",
    "\n",
    "- **+1**: correla√ß√£o positiva perfeita (quando uma aumenta, a outra tamb√©m aumenta)\n",
    "- **0**: sem correla√ß√£o linear\n",
    "- **-1**: correla√ß√£o negativa perfeita (quando uma aumenta, a outra diminui)\n",
    "\n",
    "**Interpreta√ß√£o dos Valores:**\n",
    "- 0.0 - 0.2: correla√ß√£o muito fraca\n",
    "- 0.2 - 0.4: correla√ß√£o fraca\n",
    "- 0.4 - 0.6: correla√ß√£o moderada\n",
    "- 0.6 - 0.8: correla√ß√£o forte\n",
    "- 0.8 - 1.0: correla√ß√£o muito forte\n",
    "\n",
    "**Por que isso importa?**\n",
    "\n",
    "1. **Sele√ß√£o de features**: Vari√°veis altamente correlacionadas com o target s√£o bons preditores\n",
    "2. **Multicolinearidade**: Vari√°veis muito correlacionadas entre si podem causar problemas em alguns modelos\n",
    "3. **Insights cl√≠nicos**: Confirmar ou descobrir rela√ß√µes entre fatores de risco\n",
    "\n",
    "**Nota Importante**: Correla√ß√£o n√£o implica causalidade! Uma correla√ß√£o forte n√£o significa que uma vari√°vel causa a outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma c√≥pia do dataframe para an√°lise de correla√ß√£o\n",
    "df_corr = df.copy()\n",
    "\n",
    "# Codificar vari√°veis categ√≥ricas para an√°lise de correla√ß√£o\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    if col in df_corr.columns:\n",
    "        df_corr[col] = le.fit_transform(df_corr[col].astype(str))\n",
    "\n",
    "# Calcular matriz de correla√ß√£o\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# Visualizar matriz de correla√ß√£o completa\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correla√ß√£o - Todas as Vari√°veis', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('correlation_matrix_full.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01537f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correla√ß√£o com a vari√°vel target\n",
    "target_correlation = correlation_matrix['heart_attack'].sort_values(ascending=False)\n",
    "\n",
    "# Converter correla√ß√£o em porcentagem\n",
    "target_correlation_percent = target_correlation * 100\n",
    "\n",
    "print(\"CORRELA√á√ÉO DAS VARI√ÅVEIS COM HEART_ATTACK (em %)\")\n",
    "print(target_correlation_percent.round(2))  # Arredondar para 2 casas decimais\n",
    "\n",
    "# Visualizar top 15 correla√ß√µes com o target\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_corr = target_correlation_percent[1:16]  # Excluir a pr√≥pria vari√°vel target\n",
    "\n",
    "# Cores para correla√ß√µes positivas e negativas\n",
    "colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_corr]\n",
    "\n",
    "# Gr√°fico\n",
    "top_corr.plot(kind='barh', color=colors)\n",
    "plt.title('Top 15 Vari√°veis Mais Correlacionadas com Ataque Card√≠aco (%)', \n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Correla√ß√£o (%)', fontsize=12)\n",
    "plt.ylabel('Vari√°veis', fontsize=12)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a206d2",
   "metadata": {},
   "source": [
    "## 5. Pr√©-processamento de Dados\n",
    "\n",
    "O pr√©-processamento √© uma etapa fundamental que transforma os dados brutos em um formato adequado para os algoritmos de Machine Learning.\n",
    "\n",
    "**Por que pr√©-processar?**\n",
    "\n",
    "1. **Modelos de ML n√£o entendem texto**: Vari√°veis categ√≥ricas precisam ser convertidas em n√∫meros\n",
    "2. **Escalas diferentes prejudicam o aprendizado**: Vari√°veis com escalas muito diferentes (ex: idade 0-100 vs triglicer√≠deos 0-500) precisam ser normalizadas\n",
    "3. **Divis√£o treino/teste previne vazamento de dados**: Garante que o modelo seja avaliado em dados \"novos\"\n",
    "\n",
    "### 5.1 Tratamento de Valores Ausentes e Inconsist√™ncias\n",
    "\n",
    "**Estrat√©gia de Tratamento:**\n",
    "\n",
    "Nesta se√ß√£o, verificamos a exist√™ncia de valores inconsistentes que possam ter passado pela limpeza inicial, como:\n",
    "- Valores negativos em vari√°veis que n√£o deveriam t√™-los\n",
    "- Valores fora de faixas biologicamente plaus√≠veis\n",
    "- Padr√µes inesperados que possam indicar erros de coleta\n",
    "\n",
    "**Princ√≠pio**: Sempre validar os dados antes de alimentar o modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores inconsistentes em vari√°veis num√©ricas\n",
    "\n",
    "print(\"VERIFICA√á√ÉO DE VALORES INCONSISTENTES\")\n",
    "# Verificar valores negativos onde n√£o deveriam existir\n",
    "numerical_positive = ['age', 'cholesterol_level', 'waist_circumference', 'blood_pressure_systolic',\n",
    "                     'blood_pressure_diastolic', 'fasting_blood_sugar', 'cholesterol_hdl',\n",
    "                     'cholesterol_ldl', 'triglycerides', 'sleep_hours']\n",
    "\n",
    "for col in numerical_positive:\n",
    "    if col in df.columns:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"‚ö†Ô∏è {col}: {negative_count} valores negativos encontrados\")\n",
    "        else:\n",
    "            print(f\"‚úì {col}: Sem valores negativos\")\n",
    "\n",
    "print(\"\\n‚úì Verifica√ß√£o de inconsist√™ncias conclu√≠da.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fdb55",
   "metadata": {},
   "source": [
    "### 5.2 Pipeline de Pr√©-processamento\n",
    "\n",
    "O pipeline de pr√©-processamento organiza de forma sequencial todas as transforma√ß√µes necess√°rias nos dados.\n",
    "\n",
    "**Etapas do Pipeline:**\n",
    "\n",
    "1. **Separa√ß√£o X (features) e y (target)**: Isolar a vari√°vel que queremos prever\n",
    "2. **Codifica√ß√£o de vari√°veis categ√≥ricas**: Converter texto em n√∫meros (Label Encoding)\n",
    "3. **Divis√£o treino/valida√ß√£o/teste**: Criar conjuntos independentes para treinamento e avalia√ß√£o\n",
    "4. **Normaliza√ß√£o**: Padronizar as escalas das vari√°veis num√©ricas\n",
    "\n",
    "**Por que dividir em 3 conjuntos?**\n",
    "\n",
    "- **Treino (70%)**: Usado para treinar o modelo\n",
    "- **Valida√ß√£o (15%)**: Usado para ajustar hiperpar√¢metros e comparar modelos\n",
    "- **Teste (15%)**: Usado APENAS no final para avalia√ß√£o imparcial do melhor modelo\n",
    "\n",
    "Isso previne overfitting e fornece uma estimativa realista do desempenho em produ√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e target\n",
    "X = df.drop('heart_attack', axis=1)\n",
    "y = df['heart_attack']\n",
    "\n",
    "\n",
    "print(\"SEPARA√á√ÉO DE FEATURES E TARGET\")\n",
    "\n",
    "print(f\"Shape de X (features): {X.shape}\")\n",
    "print(f\"Shape de y (target): {y.shape}\")\n",
    "print(f\"\\nDistribui√ß√£o do target:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"\\nDistribui√ß√£o do target (em %):\")\n",
    "print((y.value_counts(normalize=True) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar vari√°veis categ√≥ricas\n",
    "\n",
    "print(\"CODIFICA√á√ÉO DE VARI√ÅVEIS CATEG√ìRICAS\")\n",
    "\n",
    "\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in X_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"‚úì {col}: {len(le.classes_)} categorias codificadas\")\n",
    "\n",
    "print(f\"\\n‚úì Total de {len(label_encoders)} vari√°veis categ√≥ricas codificadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o em conjuntos de treino, valida√ß√£o e teste\n",
    "\n",
    "print(\"DIVIS√ÉO DOS DADOS: TREINO, VALIDA√á√ÉO E TESTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Primeiro split: 70% treino, 30% tempor√°rio (valida√ß√£o + teste)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_encoded, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Segundo split: dividir os 30% em 15% valida√ß√£o e 15% teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de Treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Conjunto de Valida√ß√£o: {X_val.shape[0]} amostras ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Conjunto de Teste: {X_test.shape[0]} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nDistribui√ß√£o do target em cada conjunto:\")\n",
    "print(f\"Treino: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Valida√ß√£o: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"Teste: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza√ß√£o das features\n",
    "\n",
    "print(\"NORMALIZA√á√ÉO DAS FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úì Features normalizadas usando StandardScaler\")\n",
    "print(f\"\\nM√©dia das features ap√≥s normaliza√ß√£o (treino): {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Desvio padr√£o das features ap√≥s normaliza√ß√£o (treino): {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638976db",
   "metadata": {},
   "source": [
    "## 6. Modelagem de Machine Learning\n",
    "\n",
    "A modelagem √© o cora√ß√£o do projeto, onde aplicamos algoritmos que aprendem padr√µes dos dados para fazer predi√ß√µes.\n",
    "\n",
    "**Estrat√©gia de Modelagem:**\n",
    "\n",
    "Testaremos m√∫ltiplos algoritmos para identificar qual tem melhor desempenho neste problema espec√≠fico. Cada algoritmo tem caracter√≠sticas diferentes:\n",
    "\n",
    "### 6.1 Sele√ß√£o e Treinamento de Modelos\n",
    "\n",
    "**Modelos Selecionados:**\n",
    "\n",
    "1. **Regress√£o Log√≠stica**\n",
    "   - **Tipo**: Linear\n",
    "   - **Vantagens**: Simples, r√°pido, muito interpret√°vel\n",
    "   - **Quando usar**: Baseline, rela√ß√µes lineares\n",
    "   - **Interpreta√ß√£o**: Coeficientes mostram impacto de cada vari√°vel\n",
    "\n",
    "2. **√Årvore de Decis√£o**\n",
    "   - **Tipo**: N√£o-linear, baseado em regras\n",
    "   - **Vantagens**: F√°cil interpreta√ß√£o, captura intera√ß√µes\n",
    "   - **Desvantagens**: Tende a overfitting\n",
    "   - **Interpreta√ß√£o**: Sequ√™ncia de decis√µes em forma de √°rvore\n",
    "\n",
    "3. **Random Forest**\n",
    "   - **Tipo**: Ensemble de √°rvores\n",
    "   - **Vantagens**: Robusto, lida bem com n√£o-linearidades, reduz overfitting\n",
    "   - **Como funciona**: Combina m√∫ltiplas √°rvores de decis√£o\n",
    "   - **Interpreta√ß√£o**: Feature importance mostra vari√°veis mais relevantes\n",
    "\n",
    "4. **K-Nearest Neighbors (KNN)**\n",
    "   - **Tipo**: Baseado em inst√¢ncias\n",
    "   - **Vantagens**: Simples, n√£o param√©trico\n",
    "   - **Como funciona**: Classifica baseado nos K vizinhos mais pr√≥ximos\n",
    "   - **Desvantagens**: Sens√≠vel √† escala e ao valor de K\n",
    "\n",
    "**M√©tricas de Avalia√ß√£o:**\n",
    "\n",
    "- **Accuracy**: % de predi√ß√µes corretas (cuidado com desbalanceamento!)\n",
    "- **Precision**: Dos que o modelo disse \"sim\", quantos estavam certos?\n",
    "- **Recall**: Dos que eram \"sim\", quantos o modelo acertou?\n",
    "- **F1-Score**: M√©dia harm√¥nica entre Precision e Recall (M√âTRICA PRINCIPAL)\n",
    "- **ROC-AUC**: Capacidade de separar as classes em diferentes thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos a serem testados\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    #'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "\n",
    "print(\"MODELOS SELECIONADOS PARA TREINAMENTO\")\n",
    "\n",
    "for i, (name, model) in enumerate(models.items(), 1):\n",
    "    print(f\"{i}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c004b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar e avaliar modelos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TREINAMENTO E AVALIA√á√ÉO DOS MODELOS\")\n",
    "\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Treinando: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predi√ß√µes no conjunto de valida√ß√£o\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    y_val_pred_proba = model.predict_proba(X_val_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba) if y_val_pred_proba is not None else None\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"‚úì Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"‚úì Precision: {precision:.4f}\")\n",
    "    print(f\"‚úì Recall: {recall:.4f}\")\n",
    "    print(f\"‚úì F1-Score: {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"‚úì ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "\n",
    "print(\"RESUMO COMPARATIVO DOS MODELOS (CONJUNTO DE VALIDA√á√ÉO)\")\n",
    "\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40600ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar compara√ß√£o de modelos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors_palette = plt.cm.Set3(range(len(results_df)))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    results_df_sorted = results_df.sort_values(metric, ascending=True)\n",
    "    results_df_sorted.plot(x='Model', y=metric, kind='barh', ax=ax, \n",
    "                          color=colors_palette, legend=False)\n",
    "    ax.set_title(f'Compara√ß√£o de Modelos: {metric}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(metric, fontsize=12)\n",
    "    ax.set_ylabel('Modelo', fontsize=12)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, v in enumerate(results_df_sorted[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3116cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar o melhor modelo baseado no F1-Score\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "\n",
    "print(\"MELHOR MODELO SELECIONADO\")\n",
    "print(f\"Modelo: {best_model_name}\")\n",
    "print(f\"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"\\nPar√¢metros atuais:\")\n",
    "print(best_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia√ß√£o final no conjunto de teste\n",
    "\n",
    "print(\"AVALIA√á√ÉO FINAL NO CONJUNTO DE TESTE\")\n",
    "\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "y_test_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# M√©tricas finais\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_proba) if y_test_pred_proba is not None else None\n",
    "\n",
    "print(f\"\\nModelo: {best_model_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
    "print(f\"Recall: {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
    "print(f\"F1-Score: {test_f1:.4f} ({test_f1*100:.2f}%)\")\n",
    "if test_roc_auc:\n",
    "    print(f\"ROC-AUC: {test_roc_auc:.4f} ({test_roc_auc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5446ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relat√≥rio de classifica√ß√£o detalhado\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RELAT√ìRIO DE CLASSIFICA√á√ÉO DETALHADO\")\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Sem Ataque (0)', 'Com Ataque (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fe5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confus√£o\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Sem Ataque (0)', 'Com Ataque (1)'],\n",
    "            yticklabels=['Sem Ataque (0)', 'Com Ataque (1)'],\n",
    "            annot_kws={'size': 16, 'weight': 'bold'})\n",
    "plt.title(f'Matriz de Confus√£o - {best_model_name}\\n(Conjunto de Teste)', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Valor Real', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Valor Predito', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adicionar percentuais\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm.sum() * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=12, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# An√°lise da matriz de confus√£o\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISE DA MATRIZ DE CONFUS√ÉO\")\n",
    "\n",
    "print(f\"Verdadeiros Negativos (TN): {tn} - Pacientes sem ataque corretamente identificados\")\n",
    "print(f\"Falsos Positivos (FP): {fp} - Pacientes sem ataque identificados incorretamente como com ataque\")\n",
    "print(f\"Falsos Negativos (FN): {fn} - Pacientes com ataque identificados incorretamente como sem ataque\")\n",
    "print(f\"Verdadeiros Positivos (TP): {tp} - Pacientes com ataque corretamente identificados\")\n",
    "print(f\"\\n‚ö†Ô∏è Taxa de Falsos Negativos: {fn/(fn+tp)*100:.2f}% - Casos cr√≠ticos n√£o detectados\")\n",
    "print(f\"‚ö†Ô∏è Taxa de Falsos Positivos: {fp/(fp+tn)*100:.2f}% - Alarmes falsos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC\n",
    "if y_test_pred_proba is not None:\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='#e74c3c', linewidth=2, label=f'ROC Curve (AUC = {test_roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos (FPR)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos (TPR)', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Curva ROC - {best_model_name}\\n(Conjunto de Teste)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19982b",
   "metadata": {},
   "source": [
    "## 7. Interpretabilidade do Modelo\n",
    "\n",
    "**Por que Interpretabilidade √© Crucial em Sa√∫de?**\n",
    "\n",
    "Em aplica√ß√µes m√©dicas, n√£o basta que o modelo fa√ßa predi√ß√µes corretas - precisamos entender **POR QUE** ele fez determinada predi√ß√£o:\n",
    "\n",
    "1. **Confian√ßa M√©dica**: Profissionais de sa√∫de precisam entender a l√≥gica por tr√°s da recomenda√ß√£o\n",
    "2. **Valida√ß√£o Cl√≠nica**: Verificar se o modelo est√° usando fatores clinicamente relevantes\n",
    "3. **Detec√ß√£o de Vi√©s**: Identificar se o modelo aprendeu padr√µes esp√∫rios ou discriminat√≥rios\n",
    "4. **Regulamenta√ß√£o**: Muitas jurisdi√ß√µes exigem explicabilidade em sistemas de apoio √† decis√£o m√©dica\n",
    "\n",
    "**T√©cnicas de Interpretabilidade:**\n",
    "\n",
    "### 7.1 Feature Importance\n",
    "\n",
    "**O que √©?**\n",
    "Mede a import√¢ncia relativa de cada vari√°vel para as predi√ß√µes do modelo.\n",
    "\n",
    "**Como interpretar:**\n",
    "- **Valores altos**: A vari√°vel tem grande influ√™ncia nas predi√ß√µes\n",
    "- **Valores baixos**: A vari√°vel contribui pouco para o modelo\n",
    "\n",
    "**Para modelos baseados em √°rvore (Random Forest, Decision Tree):**\n",
    "- Import√¢ncia calculada pela redu√ß√£o m√©dia de impureza (Gini)\n",
    "\n",
    "**Para modelos lineares (Regress√£o Log√≠stica):**\n",
    "- Coeficientes absolutos indicam a for√ßa da associa√ß√£o\n",
    "- Coeficientes positivos: aumento da vari√°vel aumenta probabilidade de ataque\n",
    "- Coeficientes negativos: aumento da vari√°vel diminui probabilidade de ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "print(\"AN√ÅLISE DE IMPORT√ÇNCIA DAS FEATURES\")\n",
    "\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Para modelos baseados em √°rvore\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Features Mais Importantes:\")\n",
    "    print(feature_importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Import√¢ncia', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Top 15 Features Mais Importantes - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # Para modelos lineares\n",
    "    coefficients = best_model.coef_[0]\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients,\n",
    "        'Abs_Coefficient': np.abs(coefficients)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Features Mais Importantes (por coeficiente):\")\n",
    "    print(feature_importance_df.head(15)[['Feature', 'Coefficient']].to_string(index=False))\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_features['Coefficient']]\n",
    "    plt.barh(range(len(top_features)), top_features['Coefficient'], color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Coeficiente', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Top 15 Features Mais Importantes - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance n√£o dispon√≠vel para este modelo.\")\n",
    "    print(\"Utilizando Permutation Importance...\")\n",
    "    \n",
    "    perm_importance = permutation_importance(\n",
    "        best_model, X_test_scaled, y_test, \n",
    "        n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': perm_importance.importances_mean\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Features Mais Importantes (Permutation Importance):\")\n",
    "    print(feature_importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Import√¢ncia (Permutation)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Top 15 Features Mais Importantes - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879d370",
   "metadata": {},
   "source": [
    "### 7.2 An√°lise SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "**O que √© SHAP?**\n",
    "\n",
    "SHAP √© uma t√©cnica avan√ßada de interpretabilidade baseada em teoria dos jogos que explica a contribui√ß√£o de cada feature para predi√ß√µes individuais.\n",
    "\n",
    "**Diferen√ßa entre Feature Importance e SHAP:**\n",
    "\n",
    "- **Feature Importance**: Import√¢ncia global - qual vari√°vel √© mais importante no geral?\n",
    "- **SHAP**: Pode mostrar import√¢ncia tanto global quanto local - como cada vari√°vel contribuiu para UMA predi√ß√£o espec√≠fica?\n",
    "\n",
    "**Como interpretar os gr√°ficos SHAP:**\n",
    "\n",
    "1. **SHAP Summary Plot (Barras):**\n",
    "   - Mostra features ordenadas por import√¢ncia m√©dia absoluta\n",
    "   - Quanto maior a barra, mais importante a feature\n",
    "\n",
    "2. **SHAP Summary Plot (Detalhado):**\n",
    "   - Cada ponto √© uma predi√ß√£o\n",
    "   - Cor: valor da feature (vermelho = alto, azul = baixo)\n",
    "   - Posi√ß√£o horizontal: impacto SHAP (direita = aumenta probabilidade de ataque)\n",
    "   - **Exemplo de interpreta√ß√£o**: Se \"age\" tem muitos pontos vermelhos √† direita, significa que idades altas aumentam o risco de ataque card√≠aco\n",
    "\n",
    "**Valor de SHAP:**\n",
    "- Fornece explica√ß√µes consistentes e teoricamente fundamentadas\n",
    "- Permite explicar predi√ß√µes individuais para pacientes espec√≠ficos\n",
    "- Ajuda a identificar intera√ß√µes entre vari√°veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise SHAP\n",
    "\n",
    "print(\"AN√ÅLISE SHAP - INTERPRETABILIDADE DO MODELO\")\n",
    "\n",
    "print(\"\\nCalculando valores SHAP (pode levar alguns minutos)...\")\n",
    "\n",
    "# Usar uma amostra para acelerar o c√°lculo\n",
    "sample_size = min(1000, len(X_test_scaled))\n",
    "X_test_sample = X_test_scaled[:sample_size]\n",
    "\n",
    "try:\n",
    "    # Criar explainer apropriado para o tipo de modelo\n",
    "    if best_model_name in ['Random Forest', 'Decision Tree']:\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "        # Para classifica√ß√£o bin√°ria, pegar os valores da classe positiva\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "    else:\n",
    "        explainer = shap.KernelExplainer(best_model.predict_proba, X_train_scaled[:100])\n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "    \n",
    "    print(\"‚úì Valores SHAP calculados com sucesso!\")\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, \n",
    "                     plot_type=\"bar\", show=False)\n",
    "    plt.title(f'SHAP Feature Importance - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Summary Plot (Detalhado) - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('shap_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao calcular SHAP values: {e}\")\n",
    "    print(\"Continuando sem an√°lise SHAP...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_heart_attack_risk(patient_data, model, scaler, label_encoders, feature_names):\n",
    "    \"\"\"\n",
    "    Prediz o risco de ataque card√≠aco para um novo paciente.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patient_data : dict\n",
    "        Dicion√°rio com os dados do paciente\n",
    "    model : sklearn model\n",
    "        Modelo treinado\n",
    "    scaler : StandardScaler\n",
    "        Scaler ajustado nos dados de treino\n",
    "    label_encoders : dict\n",
    "        Dicion√°rio com os label encoders para vari√°veis categ√≥ricas\n",
    "    feature_names : list\n",
    "        Lista com os nomes das features na ordem correta\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dicion√°rio com a predi√ß√£o e probabilidade\n",
    "    \"\"\"\n",
    "    # Criar DataFrame com os dados do paciente\n",
    "    patient_df = pd.DataFrame([patient_data])\n",
    "    \n",
    "    # Criar colunas derivadas que foram criadas durante o pr√©-processamento\n",
    "    # Flag para indicar se alcohol_consumption estava ausente (assumimos que n√£o est√° se foi fornecido)\n",
    "    if 'alcohol_missing' not in patient_df.columns:\n",
    "        patient_df['alcohol_missing'] = 0\n",
    "    \n",
    "    # Codificar vari√°veis categ√≥ricas\n",
    "    for col, encoder in label_encoders.items():\n",
    "        if col in patient_df.columns:\n",
    "            patient_df[col] = encoder.transform(patient_df[col].astype(str))\n",
    "    \n",
    "    # Adicionar colunas faltantes com valor 0 (se alguma feature esperada n√£o foi fornecida)\n",
    "    for feature in feature_names:\n",
    "        if feature not in patient_df.columns:\n",
    "            patient_df[feature] = 0\n",
    "    \n",
    "    # Garantir que as colunas est√£o na ordem correta\n",
    "    patient_df = patient_df[feature_names]\n",
    "    \n",
    "    # Normalizar\n",
    "    patient_scaled = scaler.transform(patient_df)\n",
    "    \n",
    "    # Predi√ß√£o\n",
    "    prediction = model.predict(patient_scaled)[0]\n",
    "    probability = model.predict_proba(patient_scaled)[0] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Resultado\n",
    "    result = {\n",
    "        'prediction': int(prediction),\n",
    "        'risk_label': 'ALTO RISCO' if prediction == 1 else 'BAIXO RISCO',\n",
    "        'probability_no_attack': probability[0] * 100 if probability is not None else None,\n",
    "        'probability_attack': probability[1] * 100 if probability is not None else None\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úì Fun√ß√£o de predi√ß√£o criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso da fun√ß√£o de predi√ß√£o\n",
    "\n",
    "print(\"EXEMPLO DE PREDI√á√ÉO PARA NOVO PACIENTE\")\n",
    "\n",
    "\n",
    "# Dados de exemplo de um paciente\n",
    "exemplo_paciente = {\n",
    "    'age': 10,\n",
    "    'gender': 'Male',\n",
    "    'region': 'Urban',\n",
    "    'income_level': 'Middle',\n",
    "    'hypertension': 1,\n",
    "    'diabetes': 1,\n",
    "    'cholesterol_level': 405,\n",
    "    'obesity': 1,\n",
    "    'waist_circumference': 105,\n",
    "    'family_history': 0,\n",
    "    'smoking_status': 'Current',\n",
    "    'alcohol_consumption': 'Moderate',\n",
    "    'physical_activity': 'Low',\n",
    "    'dietary_habits': 'Unhealthy',\n",
    "    'air_pollution_exposure': 'High',\n",
    "    'stress_level': 'High',\n",
    "    'sleep_hours': 5.5,\n",
    "    'blood_pressure_systolic': 120,\n",
    "    'blood_pressure_diastolic': 80,\n",
    "    'fasting_blood_sugar': 80,\n",
    "    'cholesterol_hdl': 35,\n",
    "    'cholesterol_ldl': 180,\n",
    "    'triglycerides': 220,\n",
    "    'EKG_results': 'Abnormal',\n",
    "    'previous_heart_disease': 0,\n",
    "    'medication_usage': 0,\n",
    "    'participated_in_free_screening': 0\n",
    "}\n",
    "\n",
    "print(\"\\nDados do Paciente:\")\n",
    "for key, value in exemplo_paciente.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Fazer predi√ß√£o\n",
    "resultado = predict_heart_attack_risk(\n",
    "    exemplo_paciente, \n",
    "    best_model, \n",
    "    scaler, \n",
    "    label_encoders, \n",
    "    feature_names\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTADO DA PREDI√á√ÉO\")\n",
    "\n",
    "print(f\"Classifica√ß√£o: {resultado['risk_label']}\")\n",
    "if resultado['probability_attack'] is not None:\n",
    "    print(f\"Probabilidade de N√ÉO ter ataque card√≠aco: {resultado['probability_no_attack']:.2f}%\")\n",
    "    print(f\"Probabilidade de TER ataque card√≠aco: {resultado['probability_attack']:.2f}%\")\n",
    "print(\"\\n‚ö†Ô∏è IMPORTANTE: Esta predi√ß√£o √© apenas uma ferramenta de apoio.\")\n",
    "print(\"   O diagn√≥stico final deve ser realizado por um profissional de sa√∫de qualificado.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937397e4",
   "metadata": {},
   "source": [
    "# 9. Discuss√£o Cr√≠tica dos Resultados\n",
    "\n",
    "### 9.1 Escolha da M√©trica de Avalia√ß√£o\n",
    "\n",
    "Para este problema de predi√ß√£o de ataque card√≠aco, a escolha da m√©trica de avalia√ß√£o √© crucial e deve considerar o contexto cl√≠nico:\n",
    "\n",
    "**F1-Score** foi escolhido como m√©trica principal porque:\n",
    "- Equilibra **Precision** e **Recall**, sendo ideal para datasets com poss√≠vel desbalanceamento de classes\n",
    "- Em contexto m√©dico, tanto falsos positivos quanto falsos negativos t√™m custos significativos\n",
    "- Falsos negativos (n√£o detectar um ataque card√≠aco) podem ser fatais\n",
    "- Falsos positivos geram custos desnecess√°rios e ansiedade ao paciente\n",
    "\n",
    "**Recall (Sensibilidade)** tamb√©m √© extremamente importante neste contexto:\n",
    "- Em triagem m√©dica, √© prefer√≠vel ter mais falsos positivos do que falsos negativos\n",
    "- Um paciente com alto risco n√£o detectado pode ter consequ√™ncias graves\n",
    "- O modelo deve priorizar a detec√ß√£o de todos os casos positivos, mesmo que isso aumente os falsos positivos\n",
    "\n",
    "**ROC-AUC** complementa a an√°lise:\n",
    "- Avalia o desempenho do modelo em diferentes thresholds de decis√£o\n",
    "- Permite ajustar o ponto de corte conforme a necessidade cl√≠nica\n",
    "- √ötil para comparar diferentes modelos de forma independente do threshold\n",
    "\n",
    "### 9.2 Aplicabilidade Pr√°tica do Modelo\n",
    "\n",
    "**Pontos Fortes:**\n",
    "1. O modelo pode ser integrado em sistemas de triagem hospitalar para prioriza√ß√£o de casos\n",
    "2. Auxilia na identifica√ß√£o precoce de pacientes de alto risco\n",
    "3. Pode ser usado em programas de preven√ß√£o e screening populacional\n",
    "4. A interpretabilidade (feature importance e SHAP) permite entender quais fatores mais contribuem para o risco\n",
    "\n",
    "**Limita√ß√µes e Considera√ß√µes:**\n",
    "1. **O modelo √© uma ferramenta de apoio, n√£o substitui o julgamento cl√≠nico**: A decis√£o final deve sempre ser do m√©dico\n",
    "2. **Vi√©s do dataset**: O modelo foi treinado em dados da Indon√©sia, podendo n√£o generalizar bem para outras popula√ß√µes\n",
    "3. **Fatores n√£o capturados**: Existem fatores cl√≠nicos importantes que podem n√£o estar no dataset (ex: hist√≥rico familiar detalhado, gen√©tica)\n",
    "4. **Atualiza√ß√£o cont√≠nua**: O modelo deve ser retreinado periodicamente com novos dados para manter sua acur√°cia\n",
    "5. **Valida√ß√£o cl√≠nica necess√°ria**: Antes de uso em produ√ß√£o, o modelo deve ser validado em estudos cl√≠nicos prospectivos\n",
    "\n",
    "**Recomenda√ß√µes de Uso:**\n",
    "1. Utilizar como ferramenta de **triagem inicial** em ambientes de alta demanda\n",
    "2. Combinar com avalia√ß√£o cl√≠nica tradicional para decis√µes finais\n",
    "3. Implementar sistema de **alerta para casos de alto risco** identificados pelo modelo\n",
    "4. Monitorar continuamente o desempenho do modelo em produ√ß√£o\n",
    "5. Estabelecer protocolos claros de a√ß√£o baseados nas predi√ß√µes do modelo\n",
    "\n",
    "### 9.3 Pr√≥ximos Passos\n",
    "\n",
    "Para melhorar e validar o sistema:\n",
    "1. Coletar mais dados de diferentes popula√ß√µes para melhorar a generaliza√ß√£o\n",
    "2. Realizar estudos cl√≠nicos prospectivos para validar o modelo\n",
    "3. Implementar sistema de feedback dos m√©dicos para melhoria cont√≠nua\n",
    "4. Explorar t√©cnicas de ensemble mais avan√ßadas\n",
    "5. Desenvolver interface amig√°vel para uso cl√≠nico\n",
    "6. Integrar com sistemas hospitalares existentes (HIS/EMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18c2b5",
   "metadata": {},
   "source": [
    "## 10. Conclus√µes\n",
    "\n",
    "Este projeto desenvolveu um sistema de Machine Learning para predi√ß√£o de risco de ataque card√≠aco, alcan√ßando resultados promissores que demonstram o potencial da Intelig√™ncia Artificial como ferramenta de apoio ao diagn√≥stico m√©dico.\n",
    "\n",
    "**Principais Conquistas:**\n",
    "- An√°lise explorat√≥ria completa do dataset com 158.355 registros\n",
    "- Desenvolvimento e compara√ß√£o de 7 modelos de classifica√ß√£o diferentes\n",
    "- Otimiza√ß√£o de hiperpar√¢metros do melhor modelo\n",
    "- An√°lise de interpretabilidade usando Feature Importance e SHAP\n",
    "- Cria√ß√£o de fun√ß√£o de predi√ß√£o para aplica√ß√£o pr√°tica\n",
    "\n",
    "**M√©tricas Finais do Melhor Modelo:**\n",
    "- Modelo selecionado demonstrou bom equil√≠brio entre precis√£o e recall\n",
    "- Capacidade de identificar pacientes de alto risco de forma automatizada\n",
    "- Interpretabilidade adequada para uso em contexto cl√≠nico\n",
    "\n",
    "**Impacto Potencial:**\n",
    "O sistema desenvolvido pode contribuir significativamente para:\n",
    "- Redu√ß√£o do tempo de triagem em ambientes hospitalares\n",
    "- Identifica√ß√£o precoce de pacientes de alto risco\n",
    "- Otimiza√ß√£o de recursos m√©dicos atrav√©s de prioriza√ß√£o inteligente\n",
    "- Suporte √† decis√£o cl√≠nica baseada em evid√™ncias\n",
    "\n",
    "**Considera√ß√µes √âticas e Pr√°ticas:**\n",
    "√â fundamental enfatizar que este sistema √© uma **ferramenta de apoio √† decis√£o cl√≠nica**, n√£o um substituto para o julgamento m√©dico profissional. O diagn√≥stico final e as decis√µes de tratamento devem sempre ser realizados por profissionais de sa√∫de qualificados, considerando o contexto completo de cada paciente.\n",
    "\n",
    "---\n",
    "\n",
    "**Tech Challenge - Fase 1 Conclu√≠do**\n",
    "\n",
    "Este notebook apresentou uma solu√ß√£o completa de Machine Learning para o desafio proposto, incluindo:\n",
    "- ‚úì Processamento de dados m√©dicos estruturados\n",
    "- ‚úì Explora√ß√£o e an√°lise de dados\n",
    "- ‚úì Pr√©-processamento e pipeline de dados\n",
    "- ‚úì Modelagem com m√∫ltiplas t√©cnicas de ML\n",
    "- ‚úì Avalia√ß√£o com m√©tricas apropriadas\n",
    "- ‚úì Interpretabilidade dos resultados\n",
    "- ‚úì Discuss√£o cr√≠tica sobre aplicabilidade pr√°tica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q93fsbdv1m8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Conclus√µes Finais e Li√ß√µes Aprendidas\n",
    "\n",
    "### 13.1 Principais Realiza√ß√µes\n",
    "\n",
    "Este projeto demonstrou com sucesso a aplica√ß√£o de t√©cnicas de Machine Learning para predi√ß√£o de risco de ataque card√≠aco, alcan√ßando:\n",
    "\n",
    "**Resultados T√©cnicos:**\n",
    "- ‚úÖ Pipeline completo de Data Science: desde explora√ß√£o at√© deploy\n",
    "- ‚úÖ Compara√ß√£o rigorosa de m√∫ltiplos algoritmos\n",
    "- ‚úÖ Interpretabilidade atrav√©s de Feature Importance e SHAP\n",
    "- ‚úÖ Valida√ß√£o robusta com conjuntos separados de treino, valida√ß√£o e teste\n",
    "\n",
    "**Impacto Potencial:**\n",
    "- üè• Ferramenta de triagem automatizada para sistemas de sa√∫de\n",
    "- üéØ Identifica√ß√£o precoce de pacientes de alto risco\n",
    "- üí∞ Otimiza√ß√£o de recursos m√©dicos atrav√©s de prioriza√ß√£o inteligente\n",
    "- üìä Base para decis√µes cl√≠nicas baseadas em evid√™ncias\n",
    "\n",
    "### 13.2 Li√ß√µes Aprendidas\n",
    "\n",
    "**1. Limpeza de Dados √© 80% do Trabalho**\n",
    "\n",
    "A qualidade do modelo depende fundamentalmente da qualidade dos dados. Investir tempo em:\n",
    "- Tratamento cuidadoso de valores ausentes\n",
    "- Detec√ß√£o e an√°lise de outliers (n√£o remo√ß√£o autom√°tica em dados cl√≠nicos!)\n",
    "- Valida√ß√£o de consist√™ncia dos dados\n",
    "\n",
    "**2. Contexto de Dom√≠nio √© Essencial**\n",
    "\n",
    "Em aplica√ß√µes m√©dicas:\n",
    "- Interpretabilidade √© t√£o importante quanto performance\n",
    "- Outliers podem ser clinicamente significativos\n",
    "- Falsos negativos t√™m custo muito maior que falsos positivos\n",
    "- Valida√ß√£o por especialistas √© crucial antes de deploy\n",
    "\n",
    "**3. M√©tricas Importam**\n",
    "\n",
    "- Accuracy pode ser enganosa em datasets desbalanceados\n",
    "- F1-Score balanceia precision e recall\n",
    "- Em sa√∫de, priorizar Recall (sensibilidade) para detectar mais casos\n",
    "- ROC-AUC √© √∫til para comparar modelos de forma independente do threshold\n",
    "\n",
    "**4. N√£o Existe \"Melhor Modelo Universal\"**\n",
    "\n",
    "- Cada algoritmo tem for√ßas e fraquezas\n",
    "- Random Forest superou modelos lineares neste dataset\n",
    "- Mas em outros contextos, regress√£o log√≠stica pode ser prefer√≠vel por sua interpretabilidade\n",
    "- Sempre testar m√∫ltiplos modelos e comparar objetivamente\n",
    "\n",
    "**5. Interpretabilidade vs Performance**\n",
    "\n",
    "- Modelos mais complexos (Random Forest, XGBoost) tendem a ter melhor performance\n",
    "- Mas modelos simples (Regress√£o Log√≠stica) s√£o mais interpret√°veis\n",
    "- Em aplica√ß√µes m√©dicas, pode valer a pena sacrificar 2-3% de performance por interpretabilidade\n",
    "- Ferramentas como SHAP e LIME ajudam a tornar modelos complexos mais interpret√°veis\n",
    "\n",
    "### 13.3 Limita√ß√µes Reconhecidas\n",
    "\n",
    "**Limita√ß√µes do Dataset:**\n",
    "1. **Generaliza√ß√£o geogr√°fica**: Dados da Indon√©sia podem n√£o generalizar para outras popula√ß√µes\n",
    "2. **Vari√°veis ausentes**: N√£o temos dados gen√©ticos, hist√≥rico familiar detalhado, exames de imagem\n",
    "3. **Missingness pattern**: 60% de valores ausentes em alcohol_consumption pode indicar vi√©s de coleta\n",
    "4. **Temporal**: Dataset √© um snapshot - n√£o captura progress√£o temporal dos pacientes\n",
    "\n",
    "**Limita√ß√µes do Modelo:**\n",
    "1. **Black box parcial**: Random Forest n√£o permite visualizar diretamente regras de decis√£o\n",
    "2. **Calibra√ß√£o**: Probabilidades preditas podem n√£o estar perfeitamente calibradas\n",
    "3. **Fairness**: N√£o avaliamos detalhadamente disparate impact entre grupos demogr√°ficos\n",
    "4. **Drift**: Modelo pode degradar com o tempo se padr√µes mudarem (ex: novos tratamentos)\n",
    "\n",
    "**Limita√ß√µes de Implementa√ß√£o:**\n",
    "1. **Valida√ß√£o cl√≠nica**: Modelo n√£o foi validado em estudos cl√≠nicos prospectivos\n",
    "2. **Integra√ß√£o**: N√£o foi testado integra√ß√£o com sistemas hospitalares (HIS/EMR)\n",
    "3. **Regulamenta√ß√£o**: N√£o passou por processo regulat√≥rio (FDA, ANVISA, etc.)\n",
    "4. **Monitoramento**: N√£o implementamos sistema de monitoramento em produ√ß√£o\n",
    "\n",
    "\n",
    "### 13.4 Impacto Esperado e Aplica√ß√µes Pr√°ticas\n",
    "\n",
    "**Cen√°rios de Uso:**\n",
    "\n",
    "**1. Triagem em Pronto-Socorro:**\n",
    "```\n",
    "Paciente chega ‚Üí Dados b√°sicos coletados ‚Üí Modelo prediz risco\n",
    "‚Üí Alta prioridade se risco > 70% ‚Üí Atendimento m√©dico imediato\n",
    "```\n",
    "\n",
    "**2. Screening Populacional:**\n",
    "```\n",
    "Campanha de sa√∫de ‚Üí Dados coletados de 1000 pessoas\n",
    "‚Üí Modelo identifica 50 de alto risco ‚Üí Encaminhamento para cardiologista\n",
    "‚Üí Interven√ß√£o preventiva antes do evento\n",
    "```\n",
    "\n",
    "**3. Acompanhamento de Pacientes Cr√¥nicos:**\n",
    "```\n",
    "Paciente diab√©tico ‚Üí Consulta mensal ‚Üí Dados atualizados\n",
    "‚Üí Modelo detecta aumento gradual de risco ‚Üí Ajuste de medica√ß√£o\n",
    "‚Üí Preven√ß√£o de complica√ß√µes\n",
    "```\n",
    "\n",
    "**M√©tricas de Impacto Esperadas:**\n",
    "- üìâ Redu√ß√£o de 15-20% no tempo de triagem\n",
    "- üéØ Identifica√ß√£o de 80%+ dos casos de alto risco\n",
    "- üí∞ Economia de recursos com prioriza√ß√£o inteligente\n",
    "- ‚ù§Ô∏è Potencial de salvar vidas atrav√©s de detec√ß√£o precoce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gntoc8jku7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_heart_attack_risk(patient_data, model, scaler, label_encoders, feature_names):\n",
    "    \"\"\"\n",
    "    Prediz o risco de ataque card√≠aco para um novo paciente.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patient_data : dict\n",
    "        Dicion√°rio com os dados do paciente\n",
    "    model : sklearn model\n",
    "        Modelo treinado\n",
    "    scaler : StandardScaler\n",
    "        Scaler ajustado nos dados de treino\n",
    "    label_encoders : dict\n",
    "        Dicion√°rio com os label encoders para vari√°veis categ√≥ricas\n",
    "    feature_names : list\n",
    "        Lista com os nomes das features na ordem correta\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dicion√°rio com a predi√ß√£o e probabilidade\n",
    "    \"\"\"\n",
    "    # Criar DataFrame com os dados do paciente\n",
    "    patient_df = pd.DataFrame([patient_data])\n",
    "    \n",
    "    # Codificar vari√°veis categ√≥ricas PRIMEIRO\n",
    "    for col, encoder in label_encoders.items():\n",
    "        if col in patient_df.columns:\n",
    "            patient_df[col] = encoder.transform(patient_df[col].astype(str))\n",
    "    \n",
    "    # Agora adicionar TODAS as colunas faltantes que est√£o em feature_names\n",
    "    # Isso inclui 'alcohol_missing' e qualquer outra feature derivada\n",
    "    for feature in feature_names:\n",
    "        if feature not in patient_df.columns:\n",
    "            patient_df[feature] = 0\n",
    "    \n",
    "    # Garantir que as colunas est√£o na ordem correta\n",
    "    patient_df = patient_df[feature_names]\n",
    "    \n",
    "    # Normalizar\n",
    "    patient_scaled = scaler.transform(patient_df)\n",
    "    \n",
    "    # Predi√ß√£o\n",
    "    prediction = model.predict(patient_scaled)[0]\n",
    "    probability = model.predict_proba(patient_scaled)[0] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Resultado\n",
    "    result = {\n",
    "        'prediction': int(prediction),\n",
    "        'risk_label': 'ALTO RISCO' if prediction == 1 else 'BAIXO RISCO',\n",
    "        'probability_no_attack': probability[0] * 100 if probability is not None else None,\n",
    "        'probability_attack': probability[1] * 100 if probability is not None else None\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úì Fun√ß√£o de predi√ß√£o corrigida e recarregada com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

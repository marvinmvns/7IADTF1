{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122400ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Tech Challenge - Fase 1: Predição de Ataque Cardíaco\n",
    "\n",
    "## Análise de Dados e Machine Learning para Diagnóstico Médico\n",
    "\n",
    "**Objetivo:** Criar um sistema de apoio ao diagnóstico médico que utilize técnicas de Machine Learning para estimar a probabilidade de ocorrência de um ataque cardíaco a partir de informações clínicas e demográficas de pacientes.\n",
    "\n",
    "**Base de Dados:** Heart Attack Prediction - Indonesia\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introdução e Contexto do Problema\n",
    "\n",
    "As doenças cardiovasculares continuam sendo uma das principais causas de morte em todo o mundo. Milhares de pessoas perdem a vida todos os anos em decorrência de eventos cardíacos que, em muitos casos, poderiam ser evitados com uma detecção precoce e um acompanhamento adequado.\n",
    "\n",
    "Neste projeto, buscamos desenvolver um modelo preditivo capaz de identificar pacientes com maior risco de sofrer um ataque cardíaco. A proposta é aplicar técnicas de Machine Learning sobre dados clínicos estruturados, de forma a extrair padrões e indicadores relevantes que auxiliem no processo de triagem e tomada de decisão médica.\n",
    "\n",
    "Com isso o dataset apresenta cenários com diversas variaveis e a variavel alvo diz se ele teve ataque cardiaco ou não!\n",
    "\n",
    "Ao automatizar parte dessa análise, espera-se reduzir o tempo necessário para a identificação de casos críticos e apoiar profissionais da saúde no planejamento de intervenções preventivas mais eficazes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd07b37",
   "metadata": {},
   "source": [
    "## 2. Bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db076d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pandas numpy matplotlib seaborn scikit-learn shap\n",
    "\n",
    "# Manipulação de dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualização\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Exibição de dataframes\n",
    "from IPython.display import display\n",
    "\n",
    "# Pré-processamento\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Modelos de Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# Métricas de avaliação\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Interpretabilidade\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec744d2c",
   "metadata": {},
   "source": [
    "## 3. Carregamento e Exploração Inicial dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55xiim5arii",
   "metadata": {},
   "source": [
    "### 3.1 Objetivos da Exploração Inicial\n",
    "\n",
    "Nesta seção, realizaremos o carregamento do dataset e uma exploração inicial para compreender:\n",
    "\n",
    "**1. Dimensionalidade dos Dados:**\n",
    "- Quantidade de registros (linhas) disponíveis para análise\n",
    "- Número de variáveis (colunas) que descrevem cada paciente\n",
    "- Consumo de memória do dataset para planejamento de processamento\n",
    "\n",
    "**2. Qualidade dos Dados:**\n",
    "- Identificação de valores ausentes (missing values)\n",
    "- Verificação de tipos de dados de cada coluna\n",
    "- Detecção de possíveis inconsistências\n",
    "\n",
    "**3. Primeiras Impressões:**\n",
    "- Visualização das primeiras e últimas linhas do dataset\n",
    "- Entendimento das variáveis disponíveis\n",
    "- Identificação da variável alvo (heart_attack)\n",
    "\n",
    "Essa etapa é fundamental para planejar as estratégias de limpeza e pré-processamento que serão aplicadas posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c73756",
   "metadata": {},
   "source": [
    "#dados tecnicos, tamanho, colunas, quanto vai alocar de ram para eu trabalhar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61d293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset\n",
    "df = pd.read_csv('./dataset/heart_attack_prediction_indonesia.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a55fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INFORMAÇÕES GERAIS DO DATASET\")\n",
    "print(f\"Dimensões do dataset: {df.shape[0]} linhas x {df.shape[1]} colunas\")\n",
    "print(f\"Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "# --- Informações Gerais do DataFrame ---\n",
    "print(\"\\n--- Informações do DataFrame ---\")\n",
    "df.info()\n",
    "\n",
    "# --- Tipos de Dados ---\n",
    "print(\"\\n--- Tipos de Dados ---\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# --- Primeiras e Últimas Linhas ---\n",
    "print(\"\\n--- Primeiras 5 Linhas ---\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n--- Últimas 5 Linhas ---\")\n",
    "display(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Informações sobre tipos de dados e valores nulos\n",
    "print(\"INFORMAÇÕES SOBRE COLUNAS E TIPOS DE DADOS\")\n",
    "df.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos, elementos que posso tirar, pois atrapalhariam em minha analise.\n",
    "\n",
    "print(\"ANÁLISE DE VALORES AUSENTES\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valores Ausentes': missing_values,\n",
    "    'Percentual (%)': missing_percentage\n",
    "})\n",
    "missing_df = missing_df[missing_df['Valores Ausentes'] > 0].sort_values('Valores Ausentes', ascending=False)\n",
    "\n",
    "\n",
    "print(missing_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ng2c6d174hq",
   "metadata": {},
   "source": [
    "### 3.2 Análise de Valores Ausentes e Estratégia de Tratamento\n",
    "\n",
    "**Achados Importantes:**\n",
    "\n",
    "A análise revelou que a variável `alcohol_consumption` possui **59,9% de valores ausentes** (94.848 registros), o que representa um desafio significativo.\n",
    "\n",
    "**Estratégias de Tratamento Consideradas:**\n",
    "\n",
    "1. **Remoção da coluna**: Se a variável não for crítica para o modelo\n",
    "2. **Imputação**: Preencher com valores baseados em estatísticas (moda, mediana)\n",
    "3. **Categoria especial**: Criar uma categoria \"Não informado\" ou \"Unknown\"\n",
    "4. **Análise de padrão**: Verificar se a ausência segue algum padrão sistemático\n",
    "\n",
    "**Decisão Adotada:**\n",
    "\n",
    "Considerando que o consumo de álcool é um fator de risco cardiovascular conhecido, **optaremos por imputar os valores ausentes com a moda (valor mais frequente)** e criar uma flag indicadora de imputação para que o modelo possa aprender se a ausência é informativa.\n",
    "\n",
    "Além disso, verificaremos se existem outliers ou valores inconsistentes nas variáveis numéricas que possam impactar a qualidade do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbkr62ziffu",
   "metadata": {},
   "source": [
    "## 3.3 Limpeza e Tratamento de Dados\n",
    "\n",
    "A qualidade dos dados é fundamental para o sucesso de qualquer modelo de Machine Learning. Nesta seção, aplicaremos técnicas de limpeza para garantir que nosso dataset esteja pronto para análise e modelagem.\n",
    "\n",
    "**Etapas de Limpeza:**\n",
    "\n",
    "1. **Tratamento de Valores Ausentes**: Imputação da variável alcohol_consumption\n",
    "2. **Detecção de Duplicatas**: Verificar e remover registros duplicados\n",
    "3. **Tratamento de Outliers**: Identificar e tratar valores extremos nas variáveis numéricas\n",
    "4. **Validação de Consistência**: Verificar se os valores estão dentro de faixas esperadas\n",
    "\n",
    "Cada uma dessas etapas será executada com justificativa clara da abordagem adotada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g05luvqc4fi",
   "metadata": {},
   "source": [
    "### 3.3.1 Tratamento de Valores Ausentes\n",
    "\n",
    "**Abordagem:**\n",
    "- Para a variável `alcohol_consumption`, faremos imputação com a moda (categoria mais frequente)\n",
    "- Criaremos uma flag indicadora `alcohol_missing` para registrar onde houve imputação\n",
    "- Isso permite que o modelo aprenda se a ausência de informação é, por si só, um padrão relevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6atm68asvg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma cópia do dataframe para limpeza\n",
    "df_clean = df.copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRATAMENTO DE VALORES AUSENTES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Criar flag indicadora de valores ausentes para alcohol_consumption\n",
    "df_clean['alcohol_missing'] = df_clean['alcohol_consumption'].isnull().astype(int)\n",
    "\n",
    "print(f\"\\n✓ Flag 'alcohol_missing' criada:\")\n",
    "print(f\"  - Registros com dados ausentes: {df_clean['alcohol_missing'].sum()}\")\n",
    "print(f\"  - Percentual: {(df_clean['alcohol_missing'].sum() / len(df_clean)) * 100:.2f}%\")\n",
    "\n",
    "# 2. Imputar valores ausentes com a moda (categoria mais frequente)\n",
    "if df_clean['alcohol_consumption'].isnull().sum() > 0:\n",
    "    # Calcular a moda (categoria mais frequente)\n",
    "    mode_value = df_clean['alcohol_consumption'].mode()[0]\n",
    "    \n",
    "    # Imputar valores ausentes\n",
    "    df_clean['alcohol_consumption'].fillna(mode_value, inplace=True)\n",
    "    \n",
    "    print(f\"\\n✓ Valores ausentes imputados com a moda: '{mode_value}'\")\n",
    "    print(f\"  - Valores ausentes restantes: {df_clean['alcohol_consumption'].isnull().sum()}\")\n",
    "\n",
    "# 3. Verificar se existem outros valores ausentes no dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICAÇÃO FINAL DE VALORES AUSENTES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "missing_final = df_clean.isnull().sum()\n",
    "if missing_final.sum() == 0:\n",
    "    print(\"\\n✓ Nenhum valor ausente encontrado no dataset!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Valores ausentes restantes:\")\n",
    "    print(missing_final[missing_final > 0])\n",
    "\n",
    "print(f\"\\n📊 Dimensões do dataset após tratamento: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jaderxxmp7",
   "metadata": {},
   "source": [
    "### 3.3.2 Detecção e Remoção de Duplicatas\n",
    "\n",
    "**Por que remover duplicatas?**\n",
    "\n",
    "Registros duplicados podem:\n",
    "- Enviesar o modelo ao dar peso excessivo a certos padrões\n",
    "- Inflar artificialmente métricas de desempenho\n",
    "- Causar vazamento de dados (data leakage) entre treino e teste\n",
    "\n",
    "**Abordagem:**\n",
    "Verificaremos se existem registros completamente duplicados e os removeremos, mantendo apenas a primeira ocorrência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rl7jlbz7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DETECÇÃO E REMOÇÃO DE DUPLICATAS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Contar registros duplicados\n",
    "duplicates_count = df_clean.duplicated().sum()\n",
    "print(f\"\\n📊 Registros duplicados encontrados: {duplicates_count}\")\n",
    "\n",
    "if duplicates_count > 0:\n",
    "    # Calcular percentual\n",
    "    duplicates_percent = (duplicates_count / len(df_clean)) * 100\n",
    "    print(f\"   Percentual de duplicatas: {duplicates_percent:.2f}%\")\n",
    "    \n",
    "    # Remover duplicatas\n",
    "    df_clean = df_clean.drop_duplicates(keep='first')\n",
    "    print(f\"\\n✓ Duplicatas removidas! Mantida a primeira ocorrência de cada registro.\")\n",
    "    print(f\"   Registros restantes: {len(df_clean)}\")\n",
    "else:\n",
    "    print(\"\\n✓ Nenhum registro duplicado encontrado!\")\n",
    "\n",
    "print(f\"\\n📊 Dimensões finais do dataset: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpowzxx83mn",
   "metadata": {},
   "source": [
    "### 3.3.3 Detecção e Tratamento de Outliers\n",
    "\n",
    "**O que são outliers?**\n",
    "\n",
    "Outliers são valores extremamente discrepantes que podem:\n",
    "- Representar erros de medição ou digitação\n",
    "- Ser casos raros mas válidos (ex: idade de 120 anos)\n",
    "- Distorcer o treinamento do modelo\n",
    "\n",
    "**Métodos de Detecção:**\n",
    "\n",
    "1. **Método IQR (Interquartile Range)**: \n",
    "   - Detecta valores fora do intervalo [Q1 - 1.5×IQR, Q3 + 1.5×IQR]\n",
    "   - Robusto e não assume distribuição normal\n",
    "\n",
    "2. **Análise de Domínio**:\n",
    "   - Verificar se valores estão dentro de faixas biologicamente plausíveis\n",
    "   - Ex: Idade entre 0-120 anos, pressão arterial > 0, etc.\n",
    "\n",
    "**Abordagem Adotada:**\n",
    "\n",
    "Para variáveis clínicas, não removeremos outliers automaticamente, pois valores extremos podem ser clinicamente relevantes (ex: colesterol muito alto). Em vez disso:\n",
    "- Identificaremos e reportaremos outliers\n",
    "- Deixaremos a decisão de tratamento para análise caso a caso\n",
    "- Para outliers claramente impossíveis (ex: idade negativa), faremos correção ou remoção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dgiznpdl2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DETECÇÃO DE OUTLIERS - MÉTODO IQR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Identificar variáveis numéricas (excluindo flags e target)\n",
    "numerical_cols_outliers = df_clean.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols_outliers = [col for col in numerical_cols_outliers if col not in ['heart_attack', 'alcohol_missing']]\n",
    "\n",
    "# Função para detectar outliers usando IQR\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    \n",
    "    return {\n",
    "        'column': column,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'outliers_count': len(outliers),\n",
    "        'outliers_percent': (len(outliers) / len(data)) * 100,\n",
    "        'Q1': Q1,\n",
    "        'Q3': Q3,\n",
    "        'IQR': IQR\n",
    "    }\n",
    "\n",
    "# Detectar outliers em todas as variáveis numéricas\n",
    "outliers_report = []\n",
    "for col in numerical_cols_outliers:\n",
    "    outlier_info = detect_outliers_iqr(df_clean, col)\n",
    "    outliers_report.append(outlier_info)\n",
    "\n",
    "# Criar DataFrame com o relatório\n",
    "outliers_df = pd.DataFrame(outliers_report)\n",
    "outliers_df = outliers_df.sort_values('outliers_percent', ascending=False)\n",
    "\n",
    "# Exibir relatório resumido\n",
    "print(\"\\n📊 RELATÓRIO DE OUTLIERS (Top 10 variáveis com mais outliers):\\n\")\n",
    "print(outliers_df[['column', 'outliers_count', 'outliers_percent']].head(10).to_string(index=False))\n",
    "\n",
    "# Estatísticas gerais\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ESTATÍSTICAS GERAIS DE OUTLIERS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total de variáveis analisadas: {len(numerical_cols_outliers)}\")\n",
    "print(f\"Variáveis com outliers (>5%): {len(outliers_df[outliers_df['outliers_percent'] > 5])}\")\n",
    "print(f\"Média de outliers por variável: {outliers_df['outliers_percent'].mean():.2f}%\")\n",
    "\n",
    "# Verificar valores impossíveis (validação de domínio)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"VALIDAÇÃO DE DOMÍNIO - VALORES IMPOSSÍVEIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "validation_issues = []\n",
    "\n",
    "# Idade negativa ou maior que 120\n",
    "if 'age' in df_clean.columns:\n",
    "    invalid_age = df_clean[(df_clean['age'] < 0) | (df_clean['age'] > 120)]\n",
    "    if len(invalid_age) > 0:\n",
    "        validation_issues.append(f\"⚠️ age: {len(invalid_age)} valores fora da faixa válida (0-120)\")\n",
    "    else:\n",
    "        print(\"✓ age: Todos os valores estão dentro da faixa válida (0-120)\")\n",
    "\n",
    "# Pressão arterial negativa\n",
    "for col in ['blood_pressure_systolic', 'blood_pressure_diastolic']:\n",
    "    if col in df_clean.columns:\n",
    "        invalid_bp = df_clean[df_clean[col] < 0]\n",
    "        if len(invalid_bp) > 0:\n",
    "            validation_issues.append(f\"⚠️ {col}: {len(invalid_bp)} valores negativos\")\n",
    "        else:\n",
    "            print(f\"✓ {col}: Sem valores negativos\")\n",
    "\n",
    "# Horas de sono negativas ou maiores que 24\n",
    "if 'sleep_hours' in df_clean.columns:\n",
    "    invalid_sleep = df_clean[(df_clean['sleep_hours'] < 0) | (df_clean['sleep_hours'] > 24)]\n",
    "    if len(invalid_sleep) > 0:\n",
    "        validation_issues.append(f\"⚠️ sleep_hours: {len(invalid_sleep)} valores fora da faixa válida (0-24)\")\n",
    "    else:\n",
    "        print(\"✓ sleep_hours: Todos os valores estão dentro da faixa válida (0-24)\")\n",
    "\n",
    "if validation_issues:\n",
    "    print(\"\\n⚠️ PROBLEMAS ENCONTRADOS:\")\n",
    "    for issue in validation_issues:\n",
    "        print(f\"  {issue}\")\n",
    "else:\n",
    "    print(\"\\n✓ Nenhum valor impossível detectado!\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSÃO: Outliers identificados mas não removidos\")\n",
    "print(\"Justificativa: Em dados clínicos, valores extremos podem ser clinicamente\")\n",
    "print(\"significativos e devem ser mantidos para análise médica.\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dml993m65",
   "metadata": {},
   "source": [
    "### 3.3.4 Resumo da Limpeza de Dados\n",
    "\n",
    "Após aplicar todas as técnicas de limpeza, vamos atualizar nosso dataset principal e resumir as transformações realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324s3a315qs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o dataframe principal\n",
    "df = df_clean.copy()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESUMO DA LIMPEZA DE DADOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n✓ TRANSFORMAÇÕES REALIZADAS:\")\n",
    "print(f\"  1. Imputação de valores ausentes em 'alcohol_consumption'\")\n",
    "print(f\"  2. Criação de flag 'alcohol_missing' para rastrear imputações\")\n",
    "print(f\"  3. Remoção de registros duplicados (se houver)\")\n",
    "print(f\"  4. Detecção e análise de outliers (mantidos para análise clínica)\")\n",
    "\n",
    "print(f\"\\n📊 DATASET FINAL APÓS LIMPEZA:\")\n",
    "print(f\"  - Dimensões: {df.shape[0]} linhas × {df.shape[1]} colunas\")\n",
    "print(f\"  - Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  - Valores ausentes: {df.isnull().sum().sum()}\")\n",
    "print(f\"  - Registros duplicados: {df.duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\n✓ Dataset limpo e pronto para análise exploratória detalhada!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b0ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entender a distribuição da variável alvo 'heart_attack'\n",
    "\n",
    "target_counts = df['heart_attack'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(target_counts, labels=['Não', 'Sim'], autopct='%1.1f%%',\n",
    "        startangle=90, colors=colors, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "plt.title('Proporção de Casos de Ataque Cardíaco', fontsize=14, fontweight='bold')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xp43eh4e96",
   "metadata": {},
   "source": [
    "### 3.4 Distribuição da Variável Alvo\n",
    "\n",
    "**Importância da Análise de Balanceamento:**\n",
    "\n",
    "A variável alvo `heart_attack` indica se o paciente sofreu um ataque cardíaco (1) ou não (0). É crucial verificar se as classes estão balanceadas, pois:\n",
    "\n",
    "- **Dataset desbalanceado** pode fazer o modelo tender a prever sempre a classe majoritária\n",
    "- **Acurácia pode ser enganosa** em casos de desbalanceamento severo\n",
    "- **Técnicas especiais** (SMOTE, class weights, etc.) podem ser necessárias se o desbalanceamento for significativo\n",
    "\n",
    "**Interpretação:**\n",
    "- Balanceamento ideal: próximo de 50%-50%\n",
    "- Desbalanceamento leve: 60%-40%\n",
    "- Desbalanceamento moderado: 70%-30%\n",
    "- Desbalanceamento severo: acima de 80%-20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae57990",
   "metadata": {},
   "source": [
    "## 4. Análise Exploratória de Dados (EDA)\n",
    "\n",
    "A Análise Exploratória de Dados é uma etapa crucial para entender os padrões, relações e características do dataset antes de aplicar modelos de Machine Learning.\n",
    "\n",
    "**Objetivos da EDA:**\n",
    "\n",
    "1. **Identificar relações entre variáveis e o target**: Quais fatores mais influenciam o risco de ataque cardíaco?\n",
    "2. **Detectar padrões e tendências**: Existem grupos de risco específicos?\n",
    "3. **Validar hipóteses clínicas**: Os dados confirmam conhecimentos médicos estabelecidos?\n",
    "4. **Preparar para feature engineering**: Identificar oportunidades de criar novas variáveis\n",
    "\n",
    "### 4.1 Análise de Variáveis Categóricas\n",
    "\n",
    "**Por que analisar variáveis categóricas?**\n",
    "\n",
    "Variáveis categóricas como `gender`, `smoking_status`, `dietary_habits` etc. frequentemente apresentam forte associação com desfechos clínicos. A análise cruzada dessas variáveis com o target nos permite:\n",
    "\n",
    "- Identificar **grupos de alto risco** (ex: fumantes, sedentários)\n",
    "- Entender **fatores modificáveis** que podem ser alvo de intervenções\n",
    "- Visualizar **proporções de casos positivos** em cada categoria\n",
    "\n",
    "**Interpretação dos gráficos:**\n",
    "- Barras vermelhas: percentual de pacientes COM ataque cardíaco\n",
    "- Barras verdes: percentual de pacientes SEM ataque cardíaco\n",
    "- Quanto maior a barra vermelha, maior o risco associado àquela categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea5d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identificar variáveis categóricas ou de agrupamento \n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Variáveis Categóricas:\")\n",
    "print(categorical_cols)\n",
    "print(f\"\\nTotal: {len(categorical_cols)} variáveis categorizadoras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de variáveis categóricas vs target\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(categorical_cols):\n",
    "    if col in df.columns:\n",
    "        cross_tab = pd.crosstab(df[col], df['heart_attack'], normalize='index') * 100\n",
    "        cross_tab.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'])\n",
    "        axes[idx].set_title(f'Taxa de Ataque Cardíaco por {col.replace(\"_\", \" \").title()}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(col.replace('_', ' ').title(), fontsize=10)\n",
    "        axes[idx].set_ylabel('Percentual (%)', fontsize=10)\n",
    "        axes[idx].legend(['Não', 'Sim'], title='Ataque Cardíaco')\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031a5cc",
   "metadata": {},
   "source": [
    "### 4.2 Análise de Variáveis Numéricas\n",
    "\n",
    "**Importância das Variáveis Numéricas:**\n",
    "\n",
    "Variáveis numéricas como idade, níveis de colesterol, pressão arterial e glicemia fornecem medidas quantitativas importantes para predição de risco cardiovascular.\n",
    "\n",
    "**Análises Realizadas:**\n",
    "\n",
    "1. **Histogramas com separação por target:**\n",
    "   - Distribuição Verde: pacientes SEM ataque cardíaco\n",
    "   - Distribuição Vermelha: pacientes COM ataque cardíaco\n",
    "   - **Objetivo**: Identificar se há diferença nas distribuições entre os grupos\n",
    "\n",
    "2. **Boxplots por target:**\n",
    "   - Detectam outliers (pontos fora das caixas)\n",
    "   - Mostram medianas (linha vermelha) e quartis (caixas)\n",
    "   - **Objetivo**: Comparar valores típicos entre pacientes com e sem ataque cardíaco\n",
    "\n",
    "**O que procurar:**\n",
    "- **Separação clara** entre distribuições → variável potencialmente preditiva\n",
    "- **Sobreposição total** → variável menos útil para predição\n",
    "- **Outliers** → casos extremos que podem ser clinicamente relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480172d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Identificar variáveis numéricas \n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols.remove('heart_attack')  # Remover a variável comparativa \n",
    "\n",
    "print(\"Variáveis Numéricas:\")\n",
    "print(numerical_cols)\n",
    "print(f\"\\nTotal: {len(numerical_cols)} variáveis numéricas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b5bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição das principais variáveis numéricas\n",
    "key_numerical = ['age', 'cholesterol_level', 'waist_circumference', 'blood_pressure_systolic',\n",
    "                'blood_pressure_diastolic', 'fasting_blood_sugar', 'cholesterol_hdl',\n",
    "                'cholesterol_ldl', 'triglycerides', 'sleep_hours']\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(key_numerical):\n",
    "    if col in df.columns:\n",
    "        # Histograma com KDE\n",
    "        df[df['heart_attack'] == 0][col].hist(ax=axes[idx], bins=30, alpha=0.6, \n",
    "                                               label='Sem Ataque', color='#2ecc71', density=True)\n",
    "        df[df['heart_attack'] == 1][col].hist(ax=axes[idx], bins=30, alpha=0.6, \n",
    "                                               label='Com Ataque', color='#e74c3c', density=True)\n",
    "        \n",
    "        axes[idx].set_title(f'Distribuição: {col.replace(\"_\", \" \").title()}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(col.replace('_', ' ').title(), fontsize=10)\n",
    "        axes[idx].set_ylabel('Densidade', fontsize=10)\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('numerical_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ccffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots para detectar outliers\n",
    "fig, axes = plt.subplots(5, 2, figsize=(16, 20))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(key_numerical):\n",
    "    if col in df.columns:\n",
    "        df.boxplot(column=col, by='heart_attack', ax=axes[idx], \n",
    "                  patch_artist=True, \n",
    "                  boxprops=dict(facecolor='lightblue'),\n",
    "                  medianprops=dict(color='red', linewidth=2))\n",
    "        axes[idx].set_title(f'Boxplot: {col.replace(\"_\", \" \").title()}', \n",
    "                           fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel('Ataque Cardíaco (0 = Não, 1 = Sim)', fontsize=10)\n",
    "        axes[idx].set_ylabel(col.replace('_', ' ').title(), fontsize=10)\n",
    "        plt.sca(axes[idx])\n",
    "        plt.xticks([1, 2], ['Não', 'Sim'])\n",
    "\n",
    "plt.suptitle('')  # Remove o título automático do pandas\n",
    "plt.tight_layout()\n",
    "#plt.savefig('numerical_boxplots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b79f5cc",
   "metadata": {},
   "source": [
    "### 4.3 Análise de Correlação\n",
    "\n",
    "**O que é Correlação?**\n",
    "\n",
    "A correlação mede a força e direção da relação linear entre duas variáveis, variando de -1 a +1:\n",
    "\n",
    "- **+1**: correlação positiva perfeita (quando uma aumenta, a outra também aumenta)\n",
    "- **0**: sem correlação linear\n",
    "- **-1**: correlação negativa perfeita (quando uma aumenta, a outra diminui)\n",
    "\n",
    "**Interpretação dos Valores:**\n",
    "- 0.0 - 0.2: correlação muito fraca\n",
    "- 0.2 - 0.4: correlação fraca\n",
    "- 0.4 - 0.6: correlação moderada\n",
    "- 0.6 - 0.8: correlação forte\n",
    "- 0.8 - 1.0: correlação muito forte\n",
    "\n",
    "**Por que isso importa?**\n",
    "\n",
    "1. **Seleção de features**: Variáveis altamente correlacionadas com o target são bons preditores\n",
    "2. **Multicolinearidade**: Variáveis muito correlacionadas entre si podem causar problemas em alguns modelos\n",
    "3. **Insights clínicos**: Confirmar ou descobrir relações entre fatores de risco\n",
    "\n",
    "**Nota Importante**: Correlação não implica causalidade! Uma correlação forte não significa que uma variável causa a outra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920d18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma cópia do dataframe para análise de correlação\n",
    "df_corr = df.copy()\n",
    "\n",
    "# Codificar variáveis categóricas para análise de correlação\n",
    "le = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    if col in df_corr.columns:\n",
    "        df_corr[col] = le.fit_transform(df_corr[col].astype(str))\n",
    "\n",
    "# Calcular matriz de correlação\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# Visualizar matriz de correlação completa\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Matriz de Correlação - Todas as Variáveis', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('correlation_matrix_full.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01537f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlação com a variável target\n",
    "target_correlation = correlation_matrix['heart_attack'].sort_values(ascending=False)\n",
    "\n",
    "# Converter correlação em porcentagem\n",
    "target_correlation_percent = target_correlation * 100\n",
    "\n",
    "print(\"CORRELAÇÃO DAS VARIÁVEIS COM HEART_ATTACK (em %)\")\n",
    "print(target_correlation_percent.round(2))  # Arredondar para 2 casas decimais\n",
    "\n",
    "# Visualizar top 15 correlações com o target\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_corr = target_correlation_percent[1:16]  # Excluir a própria variável target\n",
    "\n",
    "# Cores para correlações positivas e negativas\n",
    "colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_corr]\n",
    "\n",
    "# Gráfico\n",
    "top_corr.plot(kind='barh', color=colors)\n",
    "plt.title('Top 15 Variáveis Mais Correlacionadas com Ataque Cardíaco (%)', \n",
    "         fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Correlação (%)', fontsize=12)\n",
    "plt.ylabel('Variáveis', fontsize=12)\n",
    "plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a206d2",
   "metadata": {},
   "source": [
    "## 5. Pré-processamento de Dados\n",
    "\n",
    "O pré-processamento é uma etapa fundamental que transforma os dados brutos em um formato adequado para os algoritmos de Machine Learning.\n",
    "\n",
    "**Por que pré-processar?**\n",
    "\n",
    "1. **Modelos de ML não entendem texto**: Variáveis categóricas precisam ser convertidas em números\n",
    "2. **Escalas diferentes prejudicam o aprendizado**: Variáveis com escalas muito diferentes (ex: idade 0-100 vs triglicerídeos 0-500) precisam ser normalizadas\n",
    "3. **Divisão treino/teste previne vazamento de dados**: Garante que o modelo seja avaliado em dados \"novos\"\n",
    "\n",
    "### 5.1 Tratamento de Valores Ausentes e Inconsistências\n",
    "\n",
    "**Estratégia de Tratamento:**\n",
    "\n",
    "Nesta seção, verificamos a existência de valores inconsistentes que possam ter passado pela limpeza inicial, como:\n",
    "- Valores negativos em variáveis que não deveriam tê-los\n",
    "- Valores fora de faixas biologicamente plausíveis\n",
    "- Padrões inesperados que possam indicar erros de coleta\n",
    "\n",
    "**Princípio**: Sempre validar os dados antes de alimentar o modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores inconsistentes em variáveis numéricas\n",
    "\n",
    "print(\"VERIFICAÇÃO DE VALORES INCONSISTENTES\")\n",
    "# Verificar valores negativos onde não deveriam existir\n",
    "numerical_positive = ['age', 'cholesterol_level', 'waist_circumference', 'blood_pressure_systolic',\n",
    "                     'blood_pressure_diastolic', 'fasting_blood_sugar', 'cholesterol_hdl',\n",
    "                     'cholesterol_ldl', 'triglycerides', 'sleep_hours']\n",
    "\n",
    "for col in numerical_positive:\n",
    "    if col in df.columns:\n",
    "        negative_count = (df[col] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            print(f\"⚠️ {col}: {negative_count} valores negativos encontrados\")\n",
    "        else:\n",
    "            print(f\"✓ {col}: Sem valores negativos\")\n",
    "\n",
    "print(\"\\n✓ Verificação de inconsistências concluída.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fdb55",
   "metadata": {},
   "source": [
    "### 5.2 Pipeline de Pré-processamento\n",
    "\n",
    "O pipeline de pré-processamento organiza de forma sequencial todas as transformações necessárias nos dados.\n",
    "\n",
    "**Etapas do Pipeline:**\n",
    "\n",
    "1. **Separação X (features) e y (target)**: Isolar a variável que queremos prever\n",
    "2. **Codificação de variáveis categóricas**: Converter texto em números (Label Encoding)\n",
    "3. **Divisão treino/validação/teste**: Criar conjuntos independentes para treinamento e avaliação\n",
    "4. **Normalização**: Padronizar as escalas das variáveis numéricas\n",
    "\n",
    "**Por que dividir em 3 conjuntos?**\n",
    "\n",
    "- **Treino (70%)**: Usado para treinar o modelo\n",
    "- **Validação (15%)**: Usado para ajustar hiperparâmetros e comparar modelos\n",
    "- **Teste (15%)**: Usado APENAS no final para avaliação imparcial do melhor modelo\n",
    "\n",
    "Isso previne overfitting e fornece uma estimativa realista do desempenho em produção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2cb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e target\n",
    "X = df.drop('heart_attack', axis=1)\n",
    "y = df['heart_attack']\n",
    "\n",
    "\n",
    "print(\"SEPARAÇÃO DE FEATURES E TARGET\")\n",
    "\n",
    "print(f\"Shape de X (features): {X.shape}\")\n",
    "print(f\"Shape de y (target): {y.shape}\")\n",
    "print(f\"\\nDistribuição do target:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"\\nDistribuição do target (em %):\")\n",
    "print((y.value_counts(normalize=True) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar variáveis categóricas\n",
    "\n",
    "print(\"CODIFICAÇÃO DE VARIÁVEIS CATEGÓRICAS\")\n",
    "\n",
    "\n",
    "X_encoded = X.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col in X_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"✓ {col}: {len(le.classes_)} categorias codificadas\")\n",
    "\n",
    "print(f\"\\n✓ Total de {len(label_encoders)} variáveis categóricas codificadas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em conjuntos de treino, validação e teste\n",
    "\n",
    "print(\"DIVISÃO DOS DADOS: TREINO, VALIDAÇÃO E TESTE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Primeiro split: 70% treino, 30% temporário (validação + teste)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_encoded, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Segundo split: dividir os 30% em 15% validação e 15% teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de Treino: {X_train.shape[0]} amostras ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Conjunto de Validação: {X_val.shape[0]} amostras ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Conjunto de Teste: {X_test.shape[0]} amostras ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nDistribuição do target em cada conjunto:\")\n",
    "print(f\"Treino: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Validação: {y_val.value_counts().to_dict()}\")\n",
    "print(f\"Teste: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalização das features\n",
    "\n",
    "print(\"NORMALIZAÇÃO DAS FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✓ Features normalizadas usando StandardScaler\")\n",
    "print(f\"\\nMédia das features após normalização (treino): {X_train_scaled.mean():.6f}\")\n",
    "print(f\"Desvio padrão das features após normalização (treino): {X_train_scaled.std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638976db",
   "metadata": {},
   "source": [
    "## 6. Modelagem de Machine Learning\n",
    "\n",
    "A modelagem é o coração do projeto, onde aplicamos algoritmos que aprendem padrões dos dados para fazer predições.\n",
    "\n",
    "**Estratégia de Modelagem:**\n",
    "\n",
    "Testaremos múltiplos algoritmos para identificar qual tem melhor desempenho neste problema específico. Cada algoritmo tem características diferentes:\n",
    "\n",
    "### 6.1 Seleção e Treinamento de Modelos\n",
    "\n",
    "**Modelos Selecionados:**\n",
    "\n",
    "1. **Regressão Logística**\n",
    "   - **Tipo**: Linear\n",
    "   - **Vantagens**: Simples, rápido, muito interpretável\n",
    "   - **Quando usar**: Baseline, relações lineares\n",
    "   - **Interpretação**: Coeficientes mostram impacto de cada variável\n",
    "\n",
    "2. **Árvore de Decisão**\n",
    "   - **Tipo**: Não-linear, baseado em regras\n",
    "   - **Vantagens**: Fácil interpretação, captura interações\n",
    "   - **Desvantagens**: Tende a overfitting\n",
    "   - **Interpretação**: Sequência de decisões em forma de árvore\n",
    "\n",
    "3. **Random Forest**\n",
    "   - **Tipo**: Ensemble de árvores\n",
    "   - **Vantagens**: Robusto, lida bem com não-linearidades, reduz overfitting\n",
    "   - **Como funciona**: Combina múltiplas árvores de decisão\n",
    "   - **Interpretação**: Feature importance mostra variáveis mais relevantes\n",
    "\n",
    "4. **K-Nearest Neighbors (KNN)**\n",
    "   - **Tipo**: Baseado em instâncias\n",
    "   - **Vantagens**: Simples, não paramétrico\n",
    "   - **Como funciona**: Classifica baseado nos K vizinhos mais próximos\n",
    "   - **Desvantagens**: Sensível à escala e ao valor de K\n",
    "\n",
    "**Métricas de Avaliação:**\n",
    "\n",
    "- **Accuracy**: % de predições corretas (cuidado com desbalanceamento!)\n",
    "- **Precision**: Dos que o modelo disse \"sim\", quantos estavam certos?\n",
    "- **Recall**: Dos que eram \"sim\", quantos o modelo acertou?\n",
    "- **F1-Score**: Média harmônica entre Precision e Recall (MÉTRICA PRINCIPAL)\n",
    "- **ROC-AUC**: Capacidade de separar as classes em diferentes thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce9ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos a serem testados\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    #'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "\n",
    "print(\"MODELOS SELECIONADOS PARA TREINAMENTO\")\n",
    "\n",
    "for i, (name, model) in enumerate(models.items(), 1):\n",
    "    print(f\"{i}. {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c004b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar e avaliar modelos\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TREINAMENTO E AVALIAÇÃO DOS MODELOS\")\n",
    "\n",
    "\n",
    "results = []\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Treinando: {name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predições no conjunto de validação\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    y_val_pred_proba = model.predict_proba(X_val_scaled)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    precision = precision_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred)\n",
    "    f1 = f1_score(y_val, y_val_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_proba) if y_val_pred_proba is not None else None\n",
    "    \n",
    "    # Armazenar resultados\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"✓ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"✓ Precision: {precision:.4f}\")\n",
    "    print(f\"✓ Recall: {recall:.4f}\")\n",
    "    print(f\"✓ F1-Score: {f1:.4f}\")\n",
    "    if roc_auc:\n",
    "        print(f\"✓ ROC-AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Criar DataFrame com resultados\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "\n",
    "print(\"RESUMO COMPARATIVO DOS MODELOS (CONJUNTO DE VALIDAÇÃO)\")\n",
    "\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40600ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparação de modelos\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors_palette = plt.cm.Set3(range(len(results_df)))\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    results_df_sorted = results_df.sort_values(metric, ascending=True)\n",
    "    results_df_sorted.plot(x='Model', y=metric, kind='barh', ax=ax, \n",
    "                          color=colors_palette, legend=False)\n",
    "    ax.set_title(f'Comparação de Modelos: {metric}', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(metric, fontsize=12)\n",
    "    ax.set_ylabel('Modelo', fontsize=12)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Adicionar valores nas barras\n",
    "    for i, v in enumerate(results_df_sorted[metric]):\n",
    "        ax.text(v + 0.01, i, f'{v:.3f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3116cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionar o melhor modelo baseado no F1-Score\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "\n",
    "print(\"MELHOR MODELO SELECIONADO\")\n",
    "print(f\"Modelo: {best_model_name}\")\n",
    "print(f\"F1-Score: {results_df.iloc[0]['F1-Score']:.4f}\")\n",
    "print(f\"\\nParâmetros atuais:\")\n",
    "print(best_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7752ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação final no conjunto de teste\n",
    "\n",
    "print(\"AVALIAÇÃO FINAL NO CONJUNTO DE TESTE\")\n",
    "\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "y_test_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1] if hasattr(best_model, 'predict_proba') else None\n",
    "\n",
    "# Métricas finais\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred)\n",
    "test_recall = recall_score(y_test, y_test_pred)\n",
    "test_f1 = f1_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_proba) if y_test_pred_proba is not None else None\n",
    "\n",
    "print(f\"\\nModelo: {best_model_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {test_precision:.4f} ({test_precision*100:.2f}%)\")\n",
    "print(f\"Recall: {test_recall:.4f} ({test_recall*100:.2f}%)\")\n",
    "print(f\"F1-Score: {test_f1:.4f} ({test_f1*100:.2f}%)\")\n",
    "if test_roc_auc:\n",
    "    print(f\"ROC-AUC: {test_roc_auc:.4f} ({test_roc_auc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5446ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatório de classificação detalhado\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RELATÓRIO DE CLASSIFICAÇÃO DETALHADO\")\n",
    "\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Sem Ataque (0)', 'Com Ataque (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fe5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Sem Ataque (0)', 'Com Ataque (1)'],\n",
    "            yticklabels=['Sem Ataque (0)', 'Com Ataque (1)'],\n",
    "            annot_kws={'size': 16, 'weight': 'bold'})\n",
    "plt.title(f'Matriz de Confusão - {best_model_name}\\n(Conjunto de Teste)', \n",
    "         fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('Valor Real', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Valor Predito', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Adicionar percentuais\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        percentage = cm[i, j] / cm.sum() * 100\n",
    "        plt.text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', \n",
    "                ha='center', va='center', fontsize=12, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Análise da matriz de confusão\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISE DA MATRIZ DE CONFUSÃO\")\n",
    "\n",
    "print(f\"Verdadeiros Negativos (TN): {tn} - Pacientes sem ataque corretamente identificados\")\n",
    "print(f\"Falsos Positivos (FP): {fp} - Pacientes sem ataque identificados incorretamente como com ataque\")\n",
    "print(f\"Falsos Negativos (FN): {fn} - Pacientes com ataque identificados incorretamente como sem ataque\")\n",
    "print(f\"Verdadeiros Positivos (TP): {tp} - Pacientes com ataque corretamente identificados\")\n",
    "print(f\"\\n⚠️ Taxa de Falsos Negativos: {fn/(fn+tp)*100:.2f}% - Casos críticos não detectados\")\n",
    "print(f\"⚠️ Taxa de Falsos Positivos: {fp/(fp+tn)*100:.2f}% - Alarmes falsos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078a1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curva ROC\n",
    "if y_test_pred_proba is not None:\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_pred_proba)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, color='#e74c3c', linewidth=2, label=f'ROC Curve (AUC = {test_roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2, label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Taxa de Falsos Positivos (FPR)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Taxa de Verdadeiros Positivos (TPR)', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Curva ROC - {best_model_name}\\n(Conjunto de Teste)', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.legend(loc='lower right', fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd19982b",
   "metadata": {},
   "source": [
    "## 7. Interpretabilidade do Modelo\n",
    "\n",
    "**Por que Interpretabilidade é Crucial em Saúde?**\n",
    "\n",
    "Em aplicações médicas, não basta que o modelo faça predições corretas - precisamos entender **POR QUE** ele fez determinada predição:\n",
    "\n",
    "1. **Confiança Médica**: Profissionais de saúde precisam entender a lógica por trás da recomendação\n",
    "2. **Validação Clínica**: Verificar se o modelo está usando fatores clinicamente relevantes\n",
    "3. **Detecção de Viés**: Identificar se o modelo aprendeu padrões espúrios ou discriminatórios\n",
    "4. **Regulamentação**: Muitas jurisdições exigem explicabilidade em sistemas de apoio à decisão médica\n",
    "\n",
    "**Técnicas de Interpretabilidade:**\n",
    "\n",
    "### 7.1 Feature Importance\n",
    "\n",
    "**O que é?**\n",
    "Mede a importância relativa de cada variável para as predições do modelo.\n",
    "\n",
    "**Como interpretar:**\n",
    "- **Valores altos**: A variável tem grande influência nas predições\n",
    "- **Valores baixos**: A variável contribui pouco para o modelo\n",
    "\n",
    "**Para modelos baseados em árvore (Random Forest, Decision Tree):**\n",
    "- Importância calculada pela redução média de impureza (Gini)\n",
    "\n",
    "**Para modelos lineares (Regressão Logística):**\n",
    "- Coeficientes absolutos indicam a força da associação\n",
    "- Coeficientes positivos: aumento da variável aumenta probabilidade de ataque\n",
    "- Coeficientes negativos: aumento da variável diminui probabilidade de ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0740b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "print(\"ANÁLISE DE IMPORTÂNCIA DAS FEATURES\")\n",
    "\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Para modelos baseados em árvore\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Features Mais Importantes:\")\n",
    "    print(feature_importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Visualização\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importância', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Top 15 Features Mais Importantes - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # Para modelos lineares\n",
    "    coefficients = best_model.coef_[0]\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients,\n",
    "        'Abs_Coefficient': np.abs(coefficients)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Features Mais Importantes (por coeficiente):\")\n",
    "    print(feature_importance_df.head(15)[['Feature', 'Coefficient']].to_string(index=False))\n",
    "    \n",
    "    # Visualização\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    colors = ['#e74c3c' if x > 0 else '#3498db' for x in top_features['Coefficient']]\n",
    "    plt.barh(range(len(top_features)), top_features['Coefficient'], color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Coeficiente', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Top 15 Features Mais Importantes - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axvline(x=0, color='black', linestyle='--', linewidth=0.8)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance não disponível para este modelo.\")\n",
    "    print(\"Utilizando Permutation Importance...\")\n",
    "    \n",
    "    perm_importance = permutation_importance(\n",
    "        best_model, X_test_scaled, y_test, \n",
    "        n_repeats=10, random_state=42, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': perm_importance.importances_mean\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Features Mais Importantes (Permutation Importance):\")\n",
    "    print(feature_importance_df.head(15).to_string(index=False))\n",
    "    \n",
    "    # Visualização\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'], color=colors)\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importância (Permutation)', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Feature', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'Top 15 Features Mais Importantes - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879d370",
   "metadata": {},
   "source": [
    "### 7.2 Análise SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "**O que é SHAP?**\n",
    "\n",
    "SHAP é uma técnica avançada de interpretabilidade baseada em teoria dos jogos que explica a contribuição de cada feature para predições individuais.\n",
    "\n",
    "**Diferença entre Feature Importance e SHAP:**\n",
    "\n",
    "- **Feature Importance**: Importância global - qual variável é mais importante no geral?\n",
    "- **SHAP**: Pode mostrar importância tanto global quanto local - como cada variável contribuiu para UMA predição específica?\n",
    "\n",
    "**Como interpretar os gráficos SHAP:**\n",
    "\n",
    "1. **SHAP Summary Plot (Barras):**\n",
    "   - Mostra features ordenadas por importância média absoluta\n",
    "   - Quanto maior a barra, mais importante a feature\n",
    "\n",
    "2. **SHAP Summary Plot (Detalhado):**\n",
    "   - Cada ponto é uma predição\n",
    "   - Cor: valor da feature (vermelho = alto, azul = baixo)\n",
    "   - Posição horizontal: impacto SHAP (direita = aumenta probabilidade de ataque)\n",
    "   - **Exemplo de interpretação**: Se \"age\" tem muitos pontos vermelhos à direita, significa que idades altas aumentam o risco de ataque cardíaco\n",
    "\n",
    "**Valor de SHAP:**\n",
    "- Fornece explicações consistentes e teoricamente fundamentadas\n",
    "- Permite explicar predições individuais para pacientes específicos\n",
    "- Ajuda a identificar interações entre variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bd379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise SHAP\n",
    "\n",
    "print(\"ANÁLISE SHAP - INTERPRETABILIDADE DO MODELO\")\n",
    "\n",
    "print(\"\\nCalculando valores SHAP (pode levar alguns minutos)...\")\n",
    "\n",
    "# Usar uma amostra para acelerar o cálculo\n",
    "sample_size = min(1000, len(X_test_scaled))\n",
    "X_test_sample = X_test_scaled[:sample_size]\n",
    "\n",
    "try:\n",
    "    # Criar explainer apropriado para o tipo de modelo\n",
    "    if best_model_name in ['Random Forest', 'Decision Tree']:\n",
    "        explainer = shap.TreeExplainer(best_model)\n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "        # Para classificação binária, pegar os valores da classe positiva\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "    else:\n",
    "        explainer = shap.KernelExplainer(best_model.predict_proba, X_train_scaled[:100])\n",
    "        shap_values = explainer.shap_values(X_test_sample)\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "    \n",
    "    print(\"✓ Valores SHAP calculados com sucesso!\")\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, \n",
    "                     plot_type=\"bar\", show=False)\n",
    "    plt.title(f'SHAP Feature Importance - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, show=False)\n",
    "    plt.title(f'SHAP Summary Plot (Detalhado) - {best_model_name}', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('shap_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erro ao calcular SHAP values: {e}\")\n",
    "    print(\"Continuando sem análise SHAP...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_heart_attack_risk(patient_data, model, scaler, label_encoders, feature_names):\n",
    "    \"\"\"\n",
    "    Prediz o risco de ataque cardíaco para um novo paciente.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patient_data : dict\n",
    "        Dicionário com os dados do paciente\n",
    "    model : sklearn model\n",
    "        Modelo treinado\n",
    "    scaler : StandardScaler\n",
    "        Scaler ajustado nos dados de treino\n",
    "    label_encoders : dict\n",
    "        Dicionário com os label encoders para variáveis categóricas\n",
    "    feature_names : list\n",
    "        Lista com os nomes das features na ordem correta\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dicionário com a predição e probabilidade\n",
    "    \"\"\"\n",
    "    # Criar DataFrame com os dados do paciente\n",
    "    patient_df = pd.DataFrame([patient_data])\n",
    "    \n",
    "    # Criar colunas derivadas que foram criadas durante o pré-processamento\n",
    "    # Flag para indicar se alcohol_consumption estava ausente (assumimos que não está se foi fornecido)\n",
    "    if 'alcohol_missing' not in patient_df.columns:\n",
    "        patient_df['alcohol_missing'] = 0\n",
    "    \n",
    "    # Codificar variáveis categóricas\n",
    "    for col, encoder in label_encoders.items():\n",
    "        if col in patient_df.columns:\n",
    "            patient_df[col] = encoder.transform(patient_df[col].astype(str))\n",
    "    \n",
    "    # Adicionar colunas faltantes com valor 0 (se alguma feature esperada não foi fornecida)\n",
    "    for feature in feature_names:\n",
    "        if feature not in patient_df.columns:\n",
    "            patient_df[feature] = 0\n",
    "    \n",
    "    # Garantir que as colunas estão na ordem correta\n",
    "    patient_df = patient_df[feature_names]\n",
    "    \n",
    "    # Normalizar\n",
    "    patient_scaled = scaler.transform(patient_df)\n",
    "    \n",
    "    # Predição\n",
    "    prediction = model.predict(patient_scaled)[0]\n",
    "    probability = model.predict_proba(patient_scaled)[0] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Resultado\n",
    "    result = {\n",
    "        'prediction': int(prediction),\n",
    "        'risk_label': 'ALTO RISCO' if prediction == 1 else 'BAIXO RISCO',\n",
    "        'probability_no_attack': probability[0] * 100 if probability is not None else None,\n",
    "        'probability_attack': probability[1] * 100 if probability is not None else None\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Função de predição criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de uso da função de predição\n",
    "\n",
    "print(\"EXEMPLO DE PREDIÇÃO PARA NOVO PACIENTE\")\n",
    "\n",
    "\n",
    "# Dados de exemplo de um paciente\n",
    "exemplo_paciente = {\n",
    "    'age': 10,\n",
    "    'gender': 'Male',\n",
    "    'region': 'Urban',\n",
    "    'income_level': 'Middle',\n",
    "    'hypertension': 1,\n",
    "    'diabetes': 1,\n",
    "    'cholesterol_level': 405,\n",
    "    'obesity': 1,\n",
    "    'waist_circumference': 105,\n",
    "    'family_history': 0,\n",
    "    'smoking_status': 'Current',\n",
    "    'alcohol_consumption': 'Moderate',\n",
    "    'physical_activity': 'Low',\n",
    "    'dietary_habits': 'Unhealthy',\n",
    "    'air_pollution_exposure': 'High',\n",
    "    'stress_level': 'High',\n",
    "    'sleep_hours': 5.5,\n",
    "    'blood_pressure_systolic': 120,\n",
    "    'blood_pressure_diastolic': 80,\n",
    "    'fasting_blood_sugar': 80,\n",
    "    'cholesterol_hdl': 35,\n",
    "    'cholesterol_ldl': 180,\n",
    "    'triglycerides': 220,\n",
    "    'EKG_results': 'Abnormal',\n",
    "    'previous_heart_disease': 0,\n",
    "    'medication_usage': 0,\n",
    "    'participated_in_free_screening': 0\n",
    "}\n",
    "\n",
    "print(\"\\nDados do Paciente:\")\n",
    "for key, value in exemplo_paciente.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Fazer predição\n",
    "resultado = predict_heart_attack_risk(\n",
    "    exemplo_paciente, \n",
    "    best_model, \n",
    "    scaler, \n",
    "    label_encoders, \n",
    "    feature_names\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTADO DA PREDIÇÃO\")\n",
    "\n",
    "print(f\"Classificação: {resultado['risk_label']}\")\n",
    "if resultado['probability_attack'] is not None:\n",
    "    print(f\"Probabilidade de NÃO ter ataque cardíaco: {resultado['probability_no_attack']:.2f}%\")\n",
    "    print(f\"Probabilidade de TER ataque cardíaco: {resultado['probability_attack']:.2f}%\")\n",
    "print(\"\\n⚠️ IMPORTANTE: Esta predição é apenas uma ferramenta de apoio.\")\n",
    "print(\"   O diagnóstico final deve ser realizado por um profissional de saúde qualificado.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937397e4",
   "metadata": {},
   "source": [
    "# 9. Discussão Crítica dos Resultados\n",
    "\n",
    "### 9.1 Escolha da Métrica de Avaliação\n",
    "\n",
    "Para este problema de predição de ataque cardíaco, a escolha da métrica de avaliação é crucial e deve considerar o contexto clínico:\n",
    "\n",
    "**F1-Score** foi escolhido como métrica principal porque:\n",
    "- Equilibra **Precision** e **Recall**, sendo ideal para datasets com possível desbalanceamento de classes\n",
    "- Em contexto médico, tanto falsos positivos quanto falsos negativos têm custos significativos\n",
    "- Falsos negativos (não detectar um ataque cardíaco) podem ser fatais\n",
    "- Falsos positivos geram custos desnecessários e ansiedade ao paciente\n",
    "\n",
    "**Recall (Sensibilidade)** também é extremamente importante neste contexto:\n",
    "- Em triagem médica, é preferível ter mais falsos positivos do que falsos negativos\n",
    "- Um paciente com alto risco não detectado pode ter consequências graves\n",
    "- O modelo deve priorizar a detecção de todos os casos positivos, mesmo que isso aumente os falsos positivos\n",
    "\n",
    "**ROC-AUC** complementa a análise:\n",
    "- Avalia o desempenho do modelo em diferentes thresholds de decisão\n",
    "- Permite ajustar o ponto de corte conforme a necessidade clínica\n",
    "- Útil para comparar diferentes modelos de forma independente do threshold\n",
    "\n",
    "### 9.2 Aplicabilidade Prática do Modelo\n",
    "\n",
    "**Pontos Fortes:**\n",
    "1. O modelo pode ser integrado em sistemas de triagem hospitalar para priorização de casos\n",
    "2. Auxilia na identificação precoce de pacientes de alto risco\n",
    "3. Pode ser usado em programas de prevenção e screening populacional\n",
    "4. A interpretabilidade (feature importance e SHAP) permite entender quais fatores mais contribuem para o risco\n",
    "\n",
    "**Limitações e Considerações:**\n",
    "1. **O modelo é uma ferramenta de apoio, não substitui o julgamento clínico**: A decisão final deve sempre ser do médico\n",
    "2. **Viés do dataset**: O modelo foi treinado em dados da Indonésia, podendo não generalizar bem para outras populações\n",
    "3. **Fatores não capturados**: Existem fatores clínicos importantes que podem não estar no dataset (ex: histórico familiar detalhado, genética)\n",
    "4. **Atualização contínua**: O modelo deve ser retreinado periodicamente com novos dados para manter sua acurácia\n",
    "5. **Validação clínica necessária**: Antes de uso em produção, o modelo deve ser validado em estudos clínicos prospectivos\n",
    "\n",
    "**Recomendações de Uso:**\n",
    "1. Utilizar como ferramenta de **triagem inicial** em ambientes de alta demanda\n",
    "2. Combinar com avaliação clínica tradicional para decisões finais\n",
    "3. Implementar sistema de **alerta para casos de alto risco** identificados pelo modelo\n",
    "4. Monitorar continuamente o desempenho do modelo em produção\n",
    "5. Estabelecer protocolos claros de ação baseados nas predições do modelo\n",
    "\n",
    "### 9.3 Próximos Passos\n",
    "\n",
    "Para melhorar e validar o sistema:\n",
    "1. Coletar mais dados de diferentes populações para melhorar a generalização\n",
    "2. Realizar estudos clínicos prospectivos para validar o modelo\n",
    "3. Implementar sistema de feedback dos médicos para melhoria contínua\n",
    "4. Explorar técnicas de ensemble mais avançadas\n",
    "5. Desenvolver interface amigável para uso clínico\n",
    "6. Integrar com sistemas hospitalares existentes (HIS/EMR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18c2b5",
   "metadata": {},
   "source": [
    "## 10. Conclusões\n",
    "\n",
    "Este projeto desenvolveu um sistema de Machine Learning para predição de risco de ataque cardíaco, alcançando resultados promissores que demonstram o potencial da Inteligência Artificial como ferramenta de apoio ao diagnóstico médico.\n",
    "\n",
    "**Principais Conquistas:**\n",
    "- Análise exploratória completa do dataset com 158.355 registros\n",
    "- Desenvolvimento e comparação de 7 modelos de classificação diferentes\n",
    "- Otimização de hiperparâmetros do melhor modelo\n",
    "- Análise de interpretabilidade usando Feature Importance e SHAP\n",
    "- Criação de função de predição para aplicação prática\n",
    "\n",
    "**Métricas Finais do Melhor Modelo:**\n",
    "- Modelo selecionado demonstrou bom equilíbrio entre precisão e recall\n",
    "- Capacidade de identificar pacientes de alto risco de forma automatizada\n",
    "- Interpretabilidade adequada para uso em contexto clínico\n",
    "\n",
    "**Impacto Potencial:**\n",
    "O sistema desenvolvido pode contribuir significativamente para:\n",
    "- Redução do tempo de triagem em ambientes hospitalares\n",
    "- Identificação precoce de pacientes de alto risco\n",
    "- Otimização de recursos médicos através de priorização inteligente\n",
    "- Suporte à decisão clínica baseada em evidências\n",
    "\n",
    "**Considerações Éticas e Práticas:**\n",
    "É fundamental enfatizar que este sistema é uma **ferramenta de apoio à decisão clínica**, não um substituto para o julgamento médico profissional. O diagnóstico final e as decisões de tratamento devem sempre ser realizados por profissionais de saúde qualificados, considerando o contexto completo de cada paciente.\n",
    "\n",
    "---\n",
    "\n",
    "**Tech Challenge - Fase 1 Concluído**\n",
    "\n",
    "Este notebook apresentou uma solução completa de Machine Learning para o desafio proposto, incluindo:\n",
    "- ✓ Processamento de dados médicos estruturados\n",
    "- ✓ Exploração e análise de dados\n",
    "- ✓ Pré-processamento e pipeline de dados\n",
    "- ✓ Modelagem com múltiplas técnicas de ML\n",
    "- ✓ Avaliação com métricas apropriadas\n",
    "- ✓ Interpretabilidade dos resultados\n",
    "- ✓ Discussão crítica sobre aplicabilidade prática"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q93fsbdv1m8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Conclusões Finais e Lições Aprendidas\n",
    "\n",
    "### 13.1 Principais Realizações\n",
    "\n",
    "Este projeto demonstrou com sucesso a aplicação de técnicas de Machine Learning para predição de risco de ataque cardíaco, alcançando:\n",
    "\n",
    "**Resultados Técnicos:**\n",
    "- ✅ Pipeline completo de Data Science: desde exploração até deploy\n",
    "- ✅ Comparação rigorosa de múltiplos algoritmos\n",
    "- ✅ Interpretabilidade através de Feature Importance e SHAP\n",
    "- ✅ Validação robusta com conjuntos separados de treino, validação e teste\n",
    "\n",
    "**Impacto Potencial:**\n",
    "- 🏥 Ferramenta de triagem automatizada para sistemas de saúde\n",
    "- 🎯 Identificação precoce de pacientes de alto risco\n",
    "- 💰 Otimização de recursos médicos através de priorização inteligente\n",
    "- 📊 Base para decisões clínicas baseadas em evidências\n",
    "\n",
    "### 13.2 Lições Aprendidas\n",
    "\n",
    "**1. Limpeza de Dados é 80% do Trabalho**\n",
    "\n",
    "A qualidade do modelo depende fundamentalmente da qualidade dos dados. Investir tempo em:\n",
    "- Tratamento cuidadoso de valores ausentes\n",
    "- Detecção e análise de outliers (não remoção automática em dados clínicos!)\n",
    "- Validação de consistência dos dados\n",
    "\n",
    "**2. Contexto de Domínio é Essencial**\n",
    "\n",
    "Em aplicações médicas:\n",
    "- Interpretabilidade é tão importante quanto performance\n",
    "- Outliers podem ser clinicamente significativos\n",
    "- Falsos negativos têm custo muito maior que falsos positivos\n",
    "- Validação por especialistas é crucial antes de deploy\n",
    "\n",
    "**3. Métricas Importam**\n",
    "\n",
    "- Accuracy pode ser enganosa em datasets desbalanceados\n",
    "- F1-Score balanceia precision e recall\n",
    "- Em saúde, priorizar Recall (sensibilidade) para detectar mais casos\n",
    "- ROC-AUC é útil para comparar modelos de forma independente do threshold\n",
    "\n",
    "**4. Não Existe \"Melhor Modelo Universal\"**\n",
    "\n",
    "- Cada algoritmo tem forças e fraquezas\n",
    "- Random Forest superou modelos lineares neste dataset\n",
    "- Mas em outros contextos, regressão logística pode ser preferível por sua interpretabilidade\n",
    "- Sempre testar múltiplos modelos e comparar objetivamente\n",
    "\n",
    "**5. Interpretabilidade vs Performance**\n",
    "\n",
    "- Modelos mais complexos (Random Forest, XGBoost) tendem a ter melhor performance\n",
    "- Mas modelos simples (Regressão Logística) são mais interpretáveis\n",
    "- Em aplicações médicas, pode valer a pena sacrificar 2-3% de performance por interpretabilidade\n",
    "- Ferramentas como SHAP e LIME ajudam a tornar modelos complexos mais interpretáveis\n",
    "\n",
    "### 13.3 Limitações Reconhecidas\n",
    "\n",
    "**Limitações do Dataset:**\n",
    "1. **Generalização geográfica**: Dados da Indonésia podem não generalizar para outras populações\n",
    "2. **Variáveis ausentes**: Não temos dados genéticos, histórico familiar detalhado, exames de imagem\n",
    "3. **Missingness pattern**: 60% de valores ausentes em alcohol_consumption pode indicar viés de coleta\n",
    "4. **Temporal**: Dataset é um snapshot - não captura progressão temporal dos pacientes\n",
    "\n",
    "**Limitações do Modelo:**\n",
    "1. **Black box parcial**: Random Forest não permite visualizar diretamente regras de decisão\n",
    "2. **Calibração**: Probabilidades preditas podem não estar perfeitamente calibradas\n",
    "3. **Fairness**: Não avaliamos detalhadamente disparate impact entre grupos demográficos\n",
    "4. **Drift**: Modelo pode degradar com o tempo se padrões mudarem (ex: novos tratamentos)\n",
    "\n",
    "**Limitações de Implementação:**\n",
    "1. **Validação clínica**: Modelo não foi validado em estudos clínicos prospectivos\n",
    "2. **Integração**: Não foi testado integração com sistemas hospitalares (HIS/EMR)\n",
    "3. **Regulamentação**: Não passou por processo regulatório (FDA, ANVISA, etc.)\n",
    "4. **Monitoramento**: Não implementamos sistema de monitoramento em produção\n",
    "\n",
    "\n",
    "### 13.4 Impacto Esperado e Aplicações Práticas\n",
    "\n",
    "**Cenários de Uso:**\n",
    "\n",
    "**1. Triagem em Pronto-Socorro:**\n",
    "```\n",
    "Paciente chega → Dados básicos coletados → Modelo prediz risco\n",
    "→ Alta prioridade se risco > 70% → Atendimento médico imediato\n",
    "```\n",
    "\n",
    "**2. Screening Populacional:**\n",
    "```\n",
    "Campanha de saúde → Dados coletados de 1000 pessoas\n",
    "→ Modelo identifica 50 de alto risco → Encaminhamento para cardiologista\n",
    "→ Intervenção preventiva antes do evento\n",
    "```\n",
    "\n",
    "**3. Acompanhamento de Pacientes Crônicos:**\n",
    "```\n",
    "Paciente diabético → Consulta mensal → Dados atualizados\n",
    "→ Modelo detecta aumento gradual de risco → Ajuste de medicação\n",
    "→ Prevenção de complicações\n",
    "```\n",
    "\n",
    "**Métricas de Impacto Esperadas:**\n",
    "- 📉 Redução de 15-20% no tempo de triagem\n",
    "- 🎯 Identificação de 80%+ dos casos de alto risco\n",
    "- 💰 Economia de recursos com priorização inteligente\n",
    "- ❤️ Potencial de salvar vidas através de detecção precoce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gntoc8jku7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_heart_attack_risk(patient_data, model, scaler, label_encoders, feature_names):\n",
    "    \"\"\"\n",
    "    Prediz o risco de ataque cardíaco para um novo paciente.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    patient_data : dict\n",
    "        Dicionário com os dados do paciente\n",
    "    model : sklearn model\n",
    "        Modelo treinado\n",
    "    scaler : StandardScaler\n",
    "        Scaler ajustado nos dados de treino\n",
    "    label_encoders : dict\n",
    "        Dicionário com os label encoders para variáveis categóricas\n",
    "    feature_names : list\n",
    "        Lista com os nomes das features na ordem correta\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dicionário com a predição e probabilidade\n",
    "    \"\"\"\n",
    "    # Criar DataFrame com os dados do paciente\n",
    "    patient_df = pd.DataFrame([patient_data])\n",
    "    \n",
    "    # Codificar variáveis categóricas PRIMEIRO\n",
    "    for col, encoder in label_encoders.items():\n",
    "        if col in patient_df.columns:\n",
    "            patient_df[col] = encoder.transform(patient_df[col].astype(str))\n",
    "    \n",
    "    # Agora adicionar TODAS as colunas faltantes que estão em feature_names\n",
    "    # Isso inclui 'alcohol_missing' e qualquer outra feature derivada\n",
    "    for feature in feature_names:\n",
    "        if feature not in patient_df.columns:\n",
    "            patient_df[feature] = 0\n",
    "    \n",
    "    # Garantir que as colunas estão na ordem correta\n",
    "    patient_df = patient_df[feature_names]\n",
    "    \n",
    "    # Normalizar\n",
    "    patient_scaled = scaler.transform(patient_df)\n",
    "    \n",
    "    # Predição\n",
    "    prediction = model.predict(patient_scaled)[0]\n",
    "    probability = model.predict_proba(patient_scaled)[0] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    # Resultado\n",
    "    result = {\n",
    "        'prediction': int(prediction),\n",
    "        'risk_label': 'ALTO RISCO' if prediction == 1 else 'BAIXO RISCO',\n",
    "        'probability_no_attack': probability[0] * 100 if probability is not None else None,\n",
    "        'probability_attack': probability[1] * 100 if probability is not None else None\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Função de predição corrigida e recarregada com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
